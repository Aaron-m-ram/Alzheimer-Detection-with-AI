{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys # for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "# def preprocess_image(file_path, img_size):\n",
    "#     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "#     img = cv2.resize(img, img_size)\n",
    "#     img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "def preprocess_image(file_path, img_size):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "    \n",
    "    # Thresholding to remove black background\n",
    "    _, binary_image = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    largest_component_label = np.argmax(stats[1:, cv2.CC_STAT_AREA]) + 1\n",
    "    brain_mask = (labels == largest_component_label).astype(np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(brain_mask)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "target_size =(224, 224)\n",
    "\n",
    "# This function will pull from the directory and all subdirectory for the image and give it a label to the directory it is in\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Iterates through all subdirectories\n",
    "    for subdir in os.listdir(directory):\n",
    "        label = subdir #Make the subdirectory name be a label\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "\n",
    "        # Checks if the object it is looking at is a directory and if it is go into the directory and get all the files and preprocess them\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for image in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, image)\n",
    "\n",
    "                image = preprocess_image(file_path, target_size)\n",
    "\n",
    "                # Append to the arrays after preprocessing\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Image shape: (5121, 224, 224)\n",
      "Labels shape: (5121,)\n",
      "\n",
      "Test\n",
      "Image shape: (1279, 224, 224)\n",
      "Labels shape: (1279,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the directory paths for the training and test datasets\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "# single_test_dir = \"./Alzheimer_s Dataset/single_test\"\n",
    "\n",
    "# Load images and labels from the training directory\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "\n",
    "# Load images and labels from the test directory\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "\n",
    "# alz_single_images_test, alz_single_labels_test = load_images_from_directory(single_test_dir)\n",
    "\n",
    "# Print information about the training dataset\n",
    "print(\"Train\")\n",
    "print('Image shape:', alz_images_train.shape)\n",
    "print('Labels shape:', alz_labels_train.shape)\n",
    "\n",
    "# Print information about the test dataset\n",
    "print(\"\\nTest\")\n",
    "print('Image shape:', alz_images_test.shape)\n",
    "print('Labels shape:', alz_labels_test.shape)\n",
    "\n",
    "\n",
    "# print(\"\\nSingle Test\")\n",
    "# print('Image shape:', alz_single_images_test.shape)\n",
    "# print('Labels shape:', alz_single_labels_test.shape)\n",
    "\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "\n",
    "# print('Image train:', alz_single_images_test) # for debugging\n",
    "\n",
    "# The output of the shape follows this\n",
    "#  (X, X1, X2)\n",
    "# X is the number of pictures in the array   \n",
    "# X1 is the number of rows for a single picture (should be 224 since that is the scale)\n",
    "# X2 is the number of columns in each picture  (should be 224 since that is the scale)\n",
    "#  *Scale can be change to 207 since that is how the data is processed. \n",
    "# \n",
    "# When pull out the full array, you see alot of 0 at the start and end and that is because of the black around the brain\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Aaron's Algorithm </h1>\n",
    "CNN GCNN or similar neural networks that can be adjusted in between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (one-hot encoded): (5121, 4)\n",
      "Testing labels shape (one-hot encoded): (1279, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "alz_labels_train_encoded = label_encoder.fit_transform(alz_labels_train)\n",
    "alz_labels_test_encoded = label_encoder.fit_transform(alz_labels_test)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "alz_labels_train_onehot = tf.keras.utils.to_categorical(alz_labels_train_encoded, num_classes)\n",
    "alz_labels_test_onehot = tf.keras.utils.to_categorical(alz_labels_test_encoded, num_classes)\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "#print(alz_labels_train_onehot)\n",
    "\n",
    "print(\"Training labels shape (one-hot encoded):\", alz_labels_train_onehot.shape)\n",
    "print(\"Testing labels shape (one-hot encoded):\", alz_labels_test_onehot.shape)\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "\n",
    "# 0 = MildDemented\n",
    "# 1 = ModerateDemented\n",
    "# 2 = NonDemented\n",
    "# 3 = VeryMildDemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> CNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 91s 556ms/step - loss: 27.6525 - accuracy: 0.5438 - val_loss: 23.6364 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 17.0131 - accuracy: 0.6259 - val_loss: 29.0601 - val_accuracy: 0.3503 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 89s 554ms/step - loss: 15.9307 - accuracy: 0.6251 - val_loss: 18.6328 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 12.1645 - accuracy: 0.6272 - val_loss: 11.2882 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 89s 551ms/step - loss: 10.0402 - accuracy: 0.6532 - val_loss: 10.6785 - val_accuracy: 0.3511 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 89s 554ms/step - loss: 9.9922 - accuracy: 0.6585 - val_loss: 8.6916 - val_accuracy: 0.5059 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 9.1542 - accuracy: 0.6817 - val_loss: 9.3793 - val_accuracy: 0.3503 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 7.0060 - accuracy: 0.6858 - val_loss: 7.2783 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 5.2502 - accuracy: 0.7049 - val_loss: 10.9590 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 4.3454 - accuracy: 0.7286 - val_loss: 5.2986 - val_accuracy: 0.4801 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 3.7997 - accuracy: 0.7376 - val_loss: 4.3688 - val_accuracy: 0.5887 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 3.7466 - accuracy: 0.7446 - val_loss: 4.4343 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 2.8315 - accuracy: 0.7594 - val_loss: 3.2079 - val_accuracy: 0.5278 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 2.4877 - accuracy: 0.7754 - val_loss: 2.4297 - val_accuracy: 0.6443 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 2.3518 - accuracy: 0.7692 - val_loss: 3.4560 - val_accuracy: 0.4676 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 2.2917 - accuracy: 0.7950 - val_loss: 2.5672 - val_accuracy: 0.6247 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 1.3074 - accuracy: 0.8871 - val_loss: 1.5612 - val_accuracy: 0.6341 - lr: 2.0000e-04\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 0.7192 - accuracy: 0.9479 - val_loss: 1.7037 - val_accuracy: 0.6169 - lr: 2.0000e-04\n",
      "Epoch 19/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.5964 - accuracy: 0.9529 - val_loss: 1.4155 - val_accuracy: 0.6247 - lr: 2.0000e-04\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 0.5880 - accuracy: 0.9516 - val_loss: 1.3955 - val_accuracy: 0.6443 - lr: 2.0000e-04\n",
      "Epoch 21/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.5591 - accuracy: 0.9563 - val_loss: 1.5358 - val_accuracy: 0.6443 - lr: 2.0000e-04\n",
      "Epoch 22/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 0.5892 - accuracy: 0.9498 - val_loss: 1.5795 - val_accuracy: 0.6411 - lr: 2.0000e-04\n",
      "Epoch 23/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.4709 - accuracy: 0.9766 - val_loss: 1.3453 - val_accuracy: 0.6552 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "161/161 [==============================] - 90s 556ms/step - loss: 0.3461 - accuracy: 0.9865 - val_loss: 1.3467 - val_accuracy: 0.6560 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.2934 - accuracy: 0.9889 - val_loss: 1.2582 - val_accuracy: 0.6708 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "161/161 [==============================] - 90s 556ms/step - loss: 0.2940 - accuracy: 0.9859 - val_loss: 1.3571 - val_accuracy: 0.6646 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.2834 - accuracy: 0.9855 - val_loss: 1.4201 - val_accuracy: 0.6505 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.3191 - accuracy: 0.9797 - val_loss: 1.4138 - val_accuracy: 0.6693 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#3RD BEST MODEL\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                     validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                     callbacks=[early_stopping, learning_rate_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 92s 565ms/step - loss: 27.6656 - accuracy: 0.5167 - val_loss: 20.6026 - val_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 17.7674 - accuracy: 0.5536 - val_loss: 21.7103 - val_accuracy: 0.3503 - lr: 9.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 12.3870 - accuracy: 0.6091 - val_loss: 9.8986 - val_accuracy: 0.3503 - lr: 7.2900e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 91s 564ms/step - loss: 8.4417 - accuracy: 0.6516 - val_loss: 8.3055 - val_accuracy: 0.3503 - lr: 5.3144e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 4.7702 - accuracy: 0.7389 - val_loss: 4.7302 - val_accuracy: 0.3729 - lr: 3.4868e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 2.4933 - accuracy: 0.8377 - val_loss: 2.8808 - val_accuracy: 0.5395 - lr: 2.0589e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 92s 573ms/step - loss: 1.4114 - accuracy: 0.9291 - val_loss: 1.8714 - val_accuracy: 0.5794 - lr: 1.0942e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 92s 572ms/step - loss: 0.8104 - accuracy: 0.9754 - val_loss: 1.3456 - val_accuracy: 0.6443 - lr: 5.2335e-05\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 91s 568ms/step - loss: 0.4930 - accuracy: 0.9973 - val_loss: 1.1137 - val_accuracy: 0.7029 - lr: 2.2528e-05\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 90s 562ms/step - loss: 0.3667 - accuracy: 0.9996 - val_loss: 1.0643 - val_accuracy: 0.7013 - lr: 8.7280e-06\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 0.3264 - accuracy: 0.9996 - val_loss: 1.0964 - val_accuracy: 0.7076 - lr: 3.0433e-06\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 1.1213 - val_accuracy: 0.7084 - lr: 9.5501e-07\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.3087 - accuracy: 1.0000 - val_loss: 1.1284 - val_accuracy: 0.7123 - lr: 2.6972e-07\n",
      "Epoch 1/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 1.3377 - accuracy: 0.9123 - val_loss: 2.4916 - val_accuracy: 0.4324 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 1.0686 - accuracy: 0.9418 - val_loss: 2.4556 - val_accuracy: 0.5801 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.9128 - accuracy: 0.9623 - val_loss: 1.6709 - val_accuracy: 0.6325 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 91s 565ms/step - loss: 0.8884 - accuracy: 0.9609 - val_loss: 2.1240 - val_accuracy: 0.5895 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.8622 - accuracy: 0.9604 - val_loss: 2.0149 - val_accuracy: 0.6145 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.7280 - accuracy: 0.9756 - val_loss: 1.6453 - val_accuracy: 0.6575 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.6862 - accuracy: 0.9746 - val_loss: 1.8803 - val_accuracy: 0.6231 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 0.7031 - accuracy: 0.9746 - val_loss: 1.6910 - val_accuracy: 0.6615 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 0.7533 - accuracy: 0.9676 - val_loss: 1.6253 - val_accuracy: 0.6677 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 0.6380 - accuracy: 0.9799 - val_loss: 1.5865 - val_accuracy: 0.6990 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 91s 562ms/step - loss: 0.6011 - accuracy: 0.9791 - val_loss: 1.5091 - val_accuracy: 0.6677 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.5930 - accuracy: 0.9781 - val_loss: 2.5179 - val_accuracy: 0.6083 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.6175 - accuracy: 0.9772 - val_loss: 1.6544 - val_accuracy: 0.6669 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.6104 - accuracy: 0.9787 - val_loss: 1.5118 - val_accuracy: 0.6959 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# THE 2ND BEST\n",
    "# Learning rate scheduler - Exponential Decay\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "#THE BEST\n",
    "# Learning rate scheduler - Cyclic Learning Rate\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.1), kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# history = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "#                      validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "#                      callbacks=[early_stopping, learning_rate_scheduler])\n",
    "\n",
    "history_exp_decay = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_exp_decay, early_stopping])\n",
    "\n",
    "history_cyclic_lr = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_cyclic_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5121, 224, 224)\n",
      "Testing data shape: (1279, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to have rank 4\n",
    "alz_images_train_dnn = alz_images_train.reshape(-1, 224, 224, 1)\n",
    "alz_images_test_dnn = alz_images_test.reshape(-1, 224, 224, 1)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Training data shape:\", alz_images_train.shape)\n",
    "print(\"Testing data shape:\", alz_images_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 27s 166ms/step - loss: 8.8108 - accuracy: 0.4496 - val_loss: 3.7299 - val_accuracy: 0.3862 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 2.9792 - accuracy: 0.4826 - val_loss: 3.0124 - val_accuracy: 0.5223 - lr: 9.0000e-04\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 27s 166ms/step - loss: 2.0992 - accuracy: 0.4934 - val_loss: 1.9373 - val_accuracy: 0.3315 - lr: 7.2900e-04\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 26s 162ms/step - loss: 1.6142 - accuracy: 0.5044 - val_loss: 1.4650 - val_accuracy: 0.5317 - lr: 5.3144e-04\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.4068 - accuracy: 0.4926 - val_loss: 1.3298 - val_accuracy: 0.5090 - lr: 3.4868e-04\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 26s 163ms/step - loss: 1.2846 - accuracy: 0.5060 - val_loss: 1.2295 - val_accuracy: 0.4980 - lr: 2.0589e-04\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 26s 163ms/step - loss: 1.2265 - accuracy: 0.5117 - val_loss: 1.1794 - val_accuracy: 0.5192 - lr: 1.0942e-04\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 26s 164ms/step - loss: 1.1915 - accuracy: 0.5099 - val_loss: 1.2036 - val_accuracy: 0.5207 - lr: 5.2335e-05\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.1709 - accuracy: 0.5038 - val_loss: 1.1799 - val_accuracy: 0.5262 - lr: 2.2528e-05\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.1499 - accuracy: 0.5166 - val_loss: 1.2665 - val_accuracy: 0.5231 - lr: 8.7280e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(img_height, img_width, num_channels)),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,      # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10% of the width\n",
    "    height_shift_range=0.1, # Randomly shift images vertically by up to 10% of the height\n",
    "    shear_range=0.2,        # Randomly apply shear transformations\n",
    "    zoom_range=0.2,         # Randomly zoom in by up to 20%\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    fill_mode='nearest'     # Fill in newly created pixels (due to augmentation) using the nearest existing pixel\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Create augmented training data generator\n",
    "train_generator = datagen.flow(alz_images_train_dnn, alz_labels_train_onehot, batch_size=batch_size)\n",
    "\n",
    "# Train the model using the augmented data generator\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=len(alz_images_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(alz_images_test_dnn, alz_labels_test_onehot),\n",
    "                    callbacks=[early_stopping, lr_scheduler_exp_decay])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN with Graph based features</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 1s 596ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 1s 713ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generator function to yield batches of preprocessed images\n",
    "def image_generator(images, batch_size=32):\n",
    "    num_images = len(images)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, num_images)\n",
    "        \n",
    "        # Load and preprocess images for the current batch\n",
    "        batch_images = np.stack((images[start_index:end_index],) * 3, axis=-1)\n",
    "        preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(batch_images)\n",
    "        \n",
    "        yield preprocessed_images\n",
    "\n",
    "# Function to extract features from images using a pre-trained CNN and perform PCA\n",
    "def extract_features(images, batch_size=32, n_components=64):\n",
    "    feature_extractor = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    features = []\n",
    "    for batch_images in image_generator(images, batch_size=batch_size):\n",
    "        batch_features = feature_extractor.predict(batch_images)\n",
    "        batch_features_flat = batch_features.reshape(batch_features.shape[0], -1)\n",
    "        features.append(batch_features_flat)\n",
    "    \n",
    "    all_features = np.concatenate(features, axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "    \n",
    "    return reduced_features\n",
    "\n",
    "# Extract features from training and test images\n",
    "train_features = extract_features(alz_images_train)\n",
    "test_features = extract_features(alz_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute adjacency matrix based on feature similarity\n",
    "def compute_feature_similarity(features):\n",
    "    num_images = features.shape[0]\n",
    "    similarities = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarities[i, j] = cosine_similarity(features[i].reshape(1, -1), features[j].reshape(1, -1))[0, 0]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature similarities for training and test images\n",
    "train_feature_similarity = compute_feature_similarity(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_similarity = compute_feature_similarity(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.8\n",
    "\n",
    "# Construct adjacency matrix based on feature similarity\n",
    "def construct_adjacency_matrix(feature_similarity, threshold):\n",
    "    num_images = feature_similarity.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Set adjacency matrix value based on whether feature similarity is above threshold\n",
    "            if feature_similarity[i, j] >= threshold:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1  # Symmetric adjacency matrix\n",
    "    return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for training images: (5121, 5121)\n"
     ]
    }
   ],
   "source": [
    "# Compute adjacency matrix for training and test images\n",
    "train_adj_matrix = construct_adjacency_matrix(train_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for training images:\", train_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for test images: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "test_adj_matrix = construct_adjacency_matrix(test_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for test images:\", test_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features: (5121, 64)\n",
      "Shape of test_features: (1279, 64)\n",
      "\n",
      "Shape of train_features similarity: (5121, 5121)\n",
      "Shape of test_features similarity: (1279, 1279)\n",
      "\n",
      "Shape of train_adj_matrix: (5121, 5121)\n",
      "Shape of test_adj_matrix: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_features:\", train_features.shape)\n",
    "print(\"Shape of test_features:\", test_features.shape)\n",
    "\n",
    "print(\"\\nShape of train_features similarity:\", train_feature_similarity.shape)\n",
    "print(\"Shape of test_features similarity:\", test_feature_similarity.shape)\n",
    "\n",
    "print(\"\\nShape of train_adj_matrix:\", train_adj_matrix.shape)\n",
    "print(\"Shape of test_adj_matrix:\", test_adj_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images with grayscale index: (5121, 224, 224, 1)\n",
      "Shape of testing images with grayscale index: (1279, 224, 224, 1)\n",
      "Shape of training features with graph: (5121, 5185)\n",
      "Shape of testing features with graph: (1279, 1343)\n"
     ]
    }
   ],
   "source": [
    "# Images with grayscale index\n",
    "alz_images_train_with_grayscale_index = alz_images_train[..., np.newaxis]  # Add channel dimension for grayscale images\n",
    "alz_images_test_with_grayscale_index = alz_images_test[..., np.newaxis]\n",
    "\n",
    "# Integrate graph-based features\n",
    "train_features_with_graph = np.concatenate([train_features, train_adj_matrix], axis=1)\n",
    "test_features_with_graph = np.concatenate([test_features, test_adj_matrix], axis=1)\n",
    "\n",
    "print(\"Shape of training images with grayscale index:\", alz_images_train_with_grayscale_index.shape)\n",
    "print(\"Shape of testing images with grayscale index:\", alz_images_test_with_grayscale_index.shape)\n",
    "print(\"Shape of training features with graph:\", train_features_with_graph.shape)\n",
    "print(\"Shape of testing features with graph:\", test_features_with_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 50176)        0           ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " graph_input (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " adj_input (InputLayer)         [(None, 5121)]       0           []                               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 55361)        0           ['flatten_10[0][0]',             \n",
      "                                                                  'graph_input[0][0]',            \n",
      "                                                                  'adj_input[0][0]']              \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 128)          7086336     ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 64)           8256        ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 4)            260         ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,094,852\n",
      "Trainable params: 7,094,852\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "flatten_image = Flatten()(image_input)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers\n",
    "x = Dense(128, activation='relu')(concatenated_input)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model2 = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded test_adj_matrix: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad the test adjacency matrix with zeros to match the shape of the train adjacency matrix\n",
    "max_nodes = train_adj_matrix.shape[1]\n",
    "test_adj_matrix_padded = np.pad(test_adj_matrix, ((0, 0), (0, max_nodes - test_adj_matrix.shape[1])), mode='constant')\n",
    "\n",
    "# Verify the shape of the padded test adjacency matrix\n",
    "print(\"Shape of padded test_adj_matrix:\", test_adj_matrix_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 16s 95ms/step - loss: 2.8730 - accuracy: 0.4933 - val_loss: 4.3943 - val_accuracy: 0.3503\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 1.1173 - accuracy: 0.6171 - val_loss: 4.1000 - val_accuracy: 0.5004\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 1.0209 - accuracy: 0.6469 - val_loss: 1.9302 - val_accuracy: 0.3972\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 15s 92ms/step - loss: 0.6391 - accuracy: 0.7424 - val_loss: 1.9182 - val_accuracy: 0.5387\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.6414 - accuracy: 0.7442 - val_loss: 1.2701 - val_accuracy: 0.4910\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 15s 92ms/step - loss: 0.5399 - accuracy: 0.7727 - val_loss: 2.4503 - val_accuracy: 0.3886\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.5566 - accuracy: 0.7834 - val_loss: 1.8231 - val_accuracy: 0.5504\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.4219 - accuracy: 0.8303 - val_loss: 1.5968 - val_accuracy: 0.5426\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.3689 - accuracy: 0.8559 - val_loss: 2.0184 - val_accuracy: 0.5238\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 13s 81ms/step - loss: 0.3930 - accuracy: 0.8403 - val_loss: 2.5522 - val_accuracy: 0.5543\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = model2.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test cnn/graph</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded test_adj_matrix: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad the test adjacency matrix with zeros to match the shape of the train adjacency matrix\n",
    "max_nodes = train_adj_matrix.shape[1]\n",
    "test_adj_matrix_padded = np.pad(test_adj_matrix, ((0, 0), (0, max_nodes - test_adj_matrix.shape[1])), mode='constant')\n",
    "\n",
    "# Verify the shape of the padded test adjacency matrix\n",
    "print(\"Shape of padded test_adj_matrix:\", test_adj_matrix_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(image_input)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# Print model summary\n",
    "#model_complex.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 78s 484ms/step - loss: 2.3077 - accuracy: 0.4983 - val_loss: 1.8118 - val_accuracy: 0.5340 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 79s 491ms/step - loss: 1.6609 - accuracy: 0.5456 - val_loss: 1.6519 - val_accuracy: 0.5059 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 84s 525ms/step - loss: 1.4855 - accuracy: 0.5866 - val_loss: 1.5488 - val_accuracy: 0.5223 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 78s 487ms/step - loss: 1.3716 - accuracy: 0.6116 - val_loss: 1.6318 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 79s 489ms/step - loss: 1.2668 - accuracy: 0.6368 - val_loss: 1.4206 - val_accuracy: 0.5450 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 78s 485ms/step - loss: 1.1985 - accuracy: 0.6714 - val_loss: 1.4485 - val_accuracy: 0.4996 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 78s 485ms/step - loss: 1.1083 - accuracy: 0.7213 - val_loss: 1.3745 - val_accuracy: 0.5708 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 79s 490ms/step - loss: 1.0283 - accuracy: 0.7655 - val_loss: 1.5118 - val_accuracy: 0.4676 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 0.9395 - accuracy: 0.8100 - val_loss: 1.5514 - val_accuracy: 0.5731 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 77s 481ms/step - loss: 0.8671 - accuracy: 0.8387 - val_loss: 1.5782 - val_accuracy: 0.5575 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "history = model_complex.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(image_input)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex2 = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# Print model summary\n",
    "#model_complex.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 82s 507ms/step - loss: 6.1124 - accuracy: 0.4812 - val_loss: 4.6646 - val_accuracy: 0.5145 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 80s 498ms/step - loss: 3.9297 - accuracy: 0.5075 - val_loss: 3.2770 - val_accuracy: 0.5262 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 80s 497ms/step - loss: 2.8328 - accuracy: 0.5155 - val_loss: 2.4607 - val_accuracy: 0.5106 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 80s 499ms/step - loss: 2.1697 - accuracy: 0.5310 - val_loss: 1.9489 - val_accuracy: 0.5137 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 79s 494ms/step - loss: 1.7794 - accuracy: 0.5298 - val_loss: 1.6577 - val_accuracy: 0.5223 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 80s 498ms/step - loss: 1.5427 - accuracy: 0.5310 - val_loss: 1.4876 - val_accuracy: 0.5246 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 79s 493ms/step - loss: 1.3887 - accuracy: 0.5456 - val_loss: 1.3998 - val_accuracy: 0.5121 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 1.2991 - accuracy: 0.5483 - val_loss: 1.3006 - val_accuracy: 0.5278 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 79s 494ms/step - loss: 1.2185 - accuracy: 0.5632 - val_loss: 1.3055 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 1.1653 - accuracy: 0.5729 - val_loss: 1.2372 - val_accuracy: 0.5238 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 82s 510ms/step - loss: 1.1217 - accuracy: 0.5819 - val_loss: 1.2249 - val_accuracy: 0.5309 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 83s 514ms/step - loss: 1.0848 - accuracy: 0.5958 - val_loss: 1.1952 - val_accuracy: 0.5231 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 83s 514ms/step - loss: 1.0567 - accuracy: 0.6102 - val_loss: 1.2047 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 83s 518ms/step - loss: 1.0277 - accuracy: 0.6182 - val_loss: 1.2223 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 83s 515ms/step - loss: 1.0065 - accuracy: 0.6296 - val_loss: 1.2196 - val_accuracy: 0.4902 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "# Fit the model to the training data\n",
    "history2 = model_complex2.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1))(image_input)\n",
    "conv1_bn = BatchNormalization()(conv1)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1_bn)\n",
    "\n",
    "\n",
    "conv2 = Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool1)\n",
    "conv2_bn = BatchNormalization()(conv2)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2_bn)\n",
    "\n",
    "#conv3 = (maxpool2)\n",
    "\n",
    "\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex_most = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex_most.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 92s 568ms/step - loss: 8.4156 - accuracy: 0.4663 - val_loss: 6.7515 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 90s 556ms/step - loss: 5.9594 - accuracy: 0.5165 - val_loss: 5.0918 - val_accuracy: 0.5043 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 92s 572ms/step - loss: 4.6070 - accuracy: 0.5116 - val_loss: 4.1905 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 94s 585ms/step - loss: 3.6977 - accuracy: 0.5145 - val_loss: 3.2918 - val_accuracy: 0.5324 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 93s 578ms/step - loss: 3.0166 - accuracy: 0.5341 - val_loss: 2.8122 - val_accuracy: 0.5426 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 94s 585ms/step - loss: 2.5671 - accuracy: 0.5495 - val_loss: 2.4503 - val_accuracy: 0.5434 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 93s 579ms/step - loss: 2.2363 - accuracy: 0.5811 - val_loss: 2.1749 - val_accuracy: 0.5778 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 1.9468 - accuracy: 0.6274 - val_loss: 1.9843 - val_accuracy: 0.5841 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 1.8411 - accuracy: 0.5684 - val_loss: 1.8705 - val_accuracy: 0.6208 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 91s 567ms/step - loss: 1.6611 - accuracy: 0.5866 - val_loss: 1.7691 - val_accuracy: 0.5895 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 92s 573ms/step - loss: 1.5038 - accuracy: 0.6016 - val_loss: 1.6650 - val_accuracy: 0.6200 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 91s 564ms/step - loss: 1.4042 - accuracy: 0.6200 - val_loss: 1.5853 - val_accuracy: 0.6091 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 92s 572ms/step - loss: 1.3202 - accuracy: 0.6405 - val_loss: 1.5796 - val_accuracy: 0.6192 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 91s 564ms/step - loss: 1.1942 - accuracy: 0.6790 - val_loss: 1.6185 - val_accuracy: 0.6177 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 1.1463 - accuracy: 0.6881 - val_loss: 1.5448 - val_accuracy: 0.6169 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 92s 571ms/step - loss: 1.0519 - accuracy: 0.7231 - val_loss: 1.5424 - val_accuracy: 0.6161 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 1.0083 - accuracy: 0.7286 - val_loss: 1.4169 - val_accuracy: 0.6231 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 92s 570ms/step - loss: 0.9388 - accuracy: 0.7485 - val_loss: 1.4270 - val_accuracy: 0.6278 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "161/161 [==============================] - 92s 574ms/step - loss: 0.8858 - accuracy: 0.7872 - val_loss: 1.4839 - val_accuracy: 0.6099 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 92s 569ms/step - loss: 0.8282 - accuracy: 0.8024 - val_loss: 1.6303 - val_accuracy: 0.6169 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model_complex_most.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1))(image_input)\n",
    "conv1_bn = BatchNormalization()(conv1)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1_bn)\n",
    "\n",
    "\n",
    "conv2 = Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool1)\n",
    "conv2_bn = BatchNormalization()(conv2)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2_bn)\n",
    "\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool2)\n",
    "conv3_bn = BatchNormalization()(conv3)\n",
    "maxpool3 = MaxPooling2D(pool_size=(2, 2))(conv3_bn)\n",
    "\n",
    "\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex_most_hard = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex_most_hard.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 119s 732ms/step - loss: 15.5428 - accuracy: 0.5110 - val_loss: 13.4707 - val_accuracy: 0.2995 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 117s 729ms/step - loss: 10.2522 - accuracy: 0.5700 - val_loss: 11.5304 - val_accuracy: 0.0743 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 117s 728ms/step - loss: 7.6901 - accuracy: 0.6206 - val_loss: 7.4679 - val_accuracy: 0.5020 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 117s 729ms/step - loss: 5.9401 - accuracy: 0.6825 - val_loss: 5.6892 - val_accuracy: 0.5434 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 117s 727ms/step - loss: 4.7437 - accuracy: 0.7053 - val_loss: 4.5911 - val_accuracy: 0.6020 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 117s 726ms/step - loss: 3.8584 - accuracy: 0.7448 - val_loss: 4.2190 - val_accuracy: 0.5465 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 3.2983 - accuracy: 0.7536 - val_loss: 3.4480 - val_accuracy: 0.6169 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 2.7880 - accuracy: 0.7905 - val_loss: 3.0535 - val_accuracy: 0.6145 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 117s 724ms/step - loss: 2.3923 - accuracy: 0.8321 - val_loss: 3.0371 - val_accuracy: 0.5934 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 2.0724 - accuracy: 0.8520 - val_loss: 2.6565 - val_accuracy: 0.6294 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 1.8746 - accuracy: 0.8561 - val_loss: 2.5066 - val_accuracy: 0.6185 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 1.6459 - accuracy: 0.8780 - val_loss: 2.2271 - val_accuracy: 0.6357 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 1.4897 - accuracy: 0.8826 - val_loss: 2.5179 - val_accuracy: 0.6091 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 1.5383 - accuracy: 0.8740 - val_loss: 2.2071 - val_accuracy: 0.6153 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 114s 711ms/step - loss: 1.3518 - accuracy: 0.8983 - val_loss: 2.4144 - val_accuracy: 0.6075 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 1.2232 - accuracy: 0.9143 - val_loss: 2.0389 - val_accuracy: 0.6020 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 116s 718ms/step - loss: 1.1331 - accuracy: 0.9242 - val_loss: 1.8312 - val_accuracy: 0.6411 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 115s 713ms/step - loss: 1.0821 - accuracy: 0.9291 - val_loss: 2.0622 - val_accuracy: 0.6200 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "161/161 [==============================] - 116s 718ms/step - loss: 0.9732 - accuracy: 0.9442 - val_loss: 1.8301 - val_accuracy: 0.6177 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 0.9305 - accuracy: 0.9399 - val_loss: 1.9791 - val_accuracy: 0.6231 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 0.9347 - accuracy: 0.9414 - val_loss: 1.7881 - val_accuracy: 0.6310 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 0.8712 - accuracy: 0.9500 - val_loss: 1.9800 - val_accuracy: 0.6192 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 0.8220 - accuracy: 0.9481 - val_loss: 1.7369 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 0.7659 - accuracy: 0.9553 - val_loss: 1.6392 - val_accuracy: 0.6482 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "161/161 [==============================] - 116s 720ms/step - loss: 0.7183 - accuracy: 0.9584 - val_loss: 1.8521 - val_accuracy: 0.6263 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "161/161 [==============================] - 115s 713ms/step - loss: 0.7086 - accuracy: 0.9615 - val_loss: 1.8738 - val_accuracy: 0.6169 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "161/161 [==============================] - 114s 709ms/step - loss: 0.8091 - accuracy: 0.9342 - val_loss: 1.8068 - val_accuracy: 0.6231 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model_complex_most_hard.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine the desired shape\n",
    "desired_shape = (test_adj_matrix_padded.shape[0], 5121)\n",
    "\n",
    "# Calculate the amount of padding needed\n",
    "padding_width = desired_shape[1] - test_adj_matrix_padded.shape[1]\n",
    "\n",
    "# Pad the matrix\n",
    "padded_adj_matrix = np.pad(test_adj_matrix_padded, ((0, 0), (0, padding_width)), mode='constant', constant_values=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Resize the adjacency matrix to the desired shape\n",
    "resized_adj_matrix = np.resize(test_adj_matrix_padded, (test_adj_matrix_padded.shape[0], 5121))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_adj_matrix_padded: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of test_adj_matrix_padded:\", test_adj_matrix_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transpose the adjacency matrix\n",
    "transposed_adj_matrix = np.transpose(test_adj_matrix_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1279, 1279, 5121\n  y sizes: 1279\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_complex_most_hard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43malz_images_test_with_grayscale_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransposed_adj_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malz_labels_test_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# # Evaluate the model on the test data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# test_loss, test_accuracy = model_complex_most_hard.evaluate(\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     {'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_padded_adj_matrix2},\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Predict the test labels\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_complex2\u001b[38;5;241m.\u001b[39mpredict([alz_images_test_with_grayscale_index, test_features, test_adj_matrix])\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1279, 1279, 5121\n  y sizes: 1279\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model_complex_most_hard.evaluate([alz_images_test_with_grayscale_index, test_features, transposed_adj_matrix], alz_labels_test_onehot)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss, test_accuracy = model_complex_most_hard.evaluate(\n",
    "#     {'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_padded_adj_matrix2},\n",
    "#     {'output': alz_labels_test_onehot}\n",
    "# )\n",
    "\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = model_complex2.predict([alz_images_test_with_grayscale_index, test_features, test_adj_matrix])\n",
    "\n",
    "# Convert predicted labels from one-hot encoded format to categorical labels\n",
    "y_pred_categorical = np.argmax(y_pred, axis=1)\n",
    "test_labels_categorical = np.argmax(alz_labels_test_onehot, axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels_categorical, y_pred_categorical)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jay's Algorthm</h1>\n",
    "SVM and KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "def flatten_images(images):\n",
    "    return images.reshape(images.shape[0], -1)\n",
    "# Load and preprocess the images\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "# Flatten image data for KNN compatability\n",
    "alz_images_train_flat = flatten_images(alz_images_train)\n",
    "alz_images_test_flat = flatten_images(alz_images_test)\n",
    "# Initialize KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # You can tune the number of neighbors here\n",
    "# Fit KNN model on the training data\n",
    "knn.fit(alz_images_train_flat, alz_labels_train)\n",
    "# KNN model on test data\n",
    "alz_labels_pred = knn.predict(alz_images_test_flat)\n",
    "# accuracy score\n",
    "accuracy = accuracy_score(alz_labels_test, alz_labels_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(alz_labels_test, alz_labels_pred))\n",
    "# Initialize lists to store accuracies and k values\n",
    "k_values = list(range(1, 15))  # Evaluating k from 1 to 14\n",
    "accuracies = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(alz_images_train_flat, alz_labels_train)\n",
    "    alz_labels_pred = knn.predict(alz_images_test_flat)\n",
    "    accuracy = accuracy_score(alz_labels_test, alz_labels_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for k={k}: {accuracy:.2f}\")\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.title('KNN Accuracy vs Number of Neighbors')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Geoffrey's Algorithm</h1>\n",
    "Random Forest and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
