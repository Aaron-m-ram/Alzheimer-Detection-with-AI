{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys # for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "def preprocess_image(file_path, img_size):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "target_size =(224, 224)\n",
    "\n",
    "# This function will pull from the directory and all subdirectory for the image and give it a label to the directory it is in\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Iterates through all subdirectories\n",
    "    for subdir in os.listdir(directory):\n",
    "        label = subdir #Make the subdirectory name be a label\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "\n",
    "        # Checks if the object it is looking at is a directory and if it is go into the directory and get all the files and preprocess them\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for image in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, image)\n",
    "\n",
    "                image = preprocess_image(file_path, target_size)\n",
    "\n",
    "                # Append to the arrays after preprocessing\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Image shape: (5121, 224, 224)\n",
      "Labels shape: (5121,)\n",
      "\n",
      "Test\n",
      "Image shape: (1279, 224, 224)\n",
      "Labels shape: (1279,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the directory paths for the training and test datasets\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "\n",
    "# Load images and labels from the training directory\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "\n",
    "# Load images and labels from the test directory\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "\n",
    "# Print information about the training dataset\n",
    "print(\"Train\")\n",
    "print('Image shape:', alz_images_train.shape)\n",
    "print('Labels shape:', alz_labels_train.shape)\n",
    "\n",
    "# Print information about the test dataset\n",
    "print(\"\\nTest\")\n",
    "print('Image shape:', alz_images_test.shape)\n",
    "print('Labels shape:', alz_labels_test.shape)\n",
    "\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "# The output of the shape follows this\n",
    "#  (X, X1, X2)\n",
    "# X is the number of pictures in the array   \n",
    "# X1 is the number of rows for a single picture (should be 224 since that is the scale)\n",
    "# X2 is the number of columns in each picture  (should be 224 since that is the scale)\n",
    "#  *Scale can be change to 207 since that is how the data is processed. \n",
    "# \n",
    "# When pull out the full array, you see alot of 0 at the start and end and that is because of the black around the brain\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Aaron's Algorithm </h1>\n",
    "CNN GCNN or similar neural networks that can be adjusted in between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (one-hot encoded): (5121, 4)\n",
      "Testing labels shape (one-hot encoded): (1279, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "alz_labels_train_encoded = label_encoder.fit_transform(alz_labels_train)\n",
    "alz_labels_test_encoded = label_encoder.fit_transform(alz_labels_test)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "alz_labels_train_onehot = tf.keras.utils.to_categorical(alz_labels_train_encoded, num_classes)\n",
    "alz_labels_test_onehot = tf.keras.utils.to_categorical(alz_labels_test_encoded, num_classes)\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "#print(alz_labels_train_onehot)\n",
    "\n",
    "print(\"Training labels shape (one-hot encoded):\", alz_labels_train_onehot.shape)\n",
    "print(\"Testing labels shape (one-hot encoded):\", alz_labels_test_onehot.shape)\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "\n",
    "# 0 = MildDemented\n",
    "# 1 = ModerateDemented\n",
    "# 2 = NonDemented\n",
    "# 3 = VeryMildDemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 186624)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               23888000  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,907,332\n",
      "Trainable params: 23,907,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "model3 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels)), #all factors can be adjusted\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 55s 336ms/step - loss: 1.1589 - accuracy: 0.4733 - val_loss: 1.0713 - val_accuracy: 0.3534\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 55s 340ms/step - loss: 1.0159 - accuracy: 0.5026 - val_loss: 1.0826 - val_accuracy: 0.5020\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 55s 341ms/step - loss: 0.9593 - accuracy: 0.5247 - val_loss: 1.2423 - val_accuracy: 0.5020\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 54s 339ms/step - loss: 0.8850 - accuracy: 0.5688 - val_loss: 1.0281 - val_accuracy: 0.5465\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 52s 323ms/step - loss: 0.8115 - accuracy: 0.6009 - val_loss: 0.9684 - val_accuracy: 0.5504\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 54s 333ms/step - loss: 0.7391 - accuracy: 0.6317 - val_loss: 1.1631 - val_accuracy: 0.5590\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 53s 329ms/step - loss: 0.6418 - accuracy: 0.6917 - val_loss: 1.1526 - val_accuracy: 0.5747\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 53s 328ms/step - loss: 0.5623 - accuracy: 0.7280 - val_loss: 1.1348 - val_accuracy: 0.5575\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 52s 323ms/step - loss: 0.4689 - accuracy: 0.7807 - val_loss: 1.3822 - val_accuracy: 0.5207\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 52s 322ms/step - loss: 0.4171 - accuracy: 0.8121 - val_loss: 1.3267 - val_accuracy: 0.5301\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model3.fit(alz_images_train, alz_labels_train_onehot, epochs=10, batch_size=32, validation_data=(alz_images_test, alz_labels_test_onehot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 222, 222, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 394272)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               50466944  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,467,780\n",
      "Trainable params: 50,467,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "model4 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels)), #all factors can be adjusted\n",
    "    MaxPooling2D((2,2)),\n",
    "    # Conv2D(64, (3,3), activation='relu'),\n",
    "    # MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 40s 246ms/step - loss: 1.9952 - accuracy: 0.5427 - val_loss: 0.9959 - val_accuracy: 0.5262\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 39s 245ms/step - loss: 0.5251 - accuracy: 0.8125 - val_loss: 1.0899 - val_accuracy: 0.6091\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 40s 246ms/step - loss: 0.1928 - accuracy: 0.9596 - val_loss: 1.2683 - val_accuracy: 0.6099\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 40s 247ms/step - loss: 0.0609 - accuracy: 0.9955 - val_loss: 1.2875 - val_accuracy: 0.6106\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 39s 245ms/step - loss: 0.0282 - accuracy: 0.9986 - val_loss: 1.4521 - val_accuracy: 0.6411\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 40s 246ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.6972 - val_accuracy: 0.6177\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 40s 249ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.9019 - val_accuracy: 0.6177\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 39s 245ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.8219 - val_accuracy: 0.6349\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 40s 247ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.9839 - val_accuracy: 0.6317\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 40s 246ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.0210 - val_accuracy: 0.6294\n"
     ]
    }
   ],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history2 = model4.fit(alz_images_train, alz_labels_train_onehot, epochs=10, batch_size=32, validation_data=(alz_images_test, alz_labels_test_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jay's Algorthm</h1>\n",
    "SVM and KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Geoffrey's Algorithm</h1>\n",
    "Random Forest and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, img_channels)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Initialize the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, num_channels)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add flattening layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for DENSE (fully connected layer)\n",
    "\n",
    "Tunable Hyperparameters: Instead of choosing a fixed number of units, you can treat the number of units in dense layers as a hyperparameter to be tuned during model training using techniques like grid search or random search. This approach allows for more flexibility in optimizing the model's architecture for specific datasets and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization and accuracy code below\n",
    "\n",
    "# test_loss, test_accuracy = model1.evaluate(alz_images_test, alz_labels_test_onehot)\n",
    "# print('Test Loss:', test_loss)\n",
    "# print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
