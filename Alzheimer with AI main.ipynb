{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys # for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "# def preprocess_image(file_path, img_size):\n",
    "#     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "#     img = cv2.resize(img, img_size)\n",
    "#     img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "def preprocess_image(file_path, img_size):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "    \n",
    "    # Thresholding to remove black background\n",
    "    _, binary_image = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    largest_component_label = np.argmax(stats[1:, cv2.CC_STAT_AREA]) + 1\n",
    "    brain_mask = (labels == largest_component_label).astype(np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(brain_mask)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "target_size =(224, 224)\n",
    "\n",
    "# This function will pull from the directory and all subdirectory for the image and give it a label to the directory it is in\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Iterates through all subdirectories\n",
    "    for subdir in os.listdir(directory):\n",
    "        label = subdir #Make the subdirectory name be a label\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "\n",
    "        # Checks if the object it is looking at is a directory and if it is go into the directory and get all the files and preprocess them\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for image in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, image)\n",
    "\n",
    "                image = preprocess_image(file_path, target_size)\n",
    "\n",
    "                # Append to the arrays after preprocessing\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Image shape: (5121, 224, 224)\n",
      "Labels shape: (5121,)\n",
      "\n",
      "Test\n",
      "Image shape: (1279, 224, 224)\n",
      "Labels shape: (1279,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the directory paths for the training and test datasets\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "# single_test_dir = \"./Alzheimer_s Dataset/single_test\"\n",
    "\n",
    "# Load images and labels from the training directory\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "\n",
    "# Load images and labels from the test directory\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "\n",
    "# alz_single_images_test, alz_single_labels_test = load_images_from_directory(single_test_dir)\n",
    "\n",
    "# Print information about the training dataset\n",
    "print(\"Train\")\n",
    "print('Image shape:', alz_images_train.shape)\n",
    "print('Labels shape:', alz_labels_train.shape)\n",
    "\n",
    "# Print information about the test dataset\n",
    "print(\"\\nTest\")\n",
    "print('Image shape:', alz_images_test.shape)\n",
    "print('Labels shape:', alz_labels_test.shape)\n",
    "\n",
    "\n",
    "# print(\"\\nSingle Test\")\n",
    "# print('Image shape:', alz_single_images_test.shape)\n",
    "# print('Labels shape:', alz_single_labels_test.shape)\n",
    "\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "\n",
    "# print('Image train:', alz_single_images_test) # for debugging\n",
    "\n",
    "# The output of the shape follows this\n",
    "#  (X, X1, X2)\n",
    "# X is the number of pictures in the array   \n",
    "# X1 is the number of rows for a single picture (should be 224 since that is the scale)\n",
    "# X2 is the number of columns in each picture  (should be 224 since that is the scale)\n",
    "#  *Scale can be change to 207 since that is how the data is processed. \n",
    "# \n",
    "# When pull out the full array, you see alot of 0 at the start and end and that is because of the black around the brain\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Aaron's Algorithm </h1>\n",
    "CNN GCNN or similar neural networks that can be adjusted in between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (one-hot encoded): (5121, 4)\n",
      "Testing labels shape (one-hot encoded): (1279, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "alz_labels_train_encoded = label_encoder.fit_transform(alz_labels_train)\n",
    "alz_labels_test_encoded = label_encoder.fit_transform(alz_labels_test)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "alz_labels_train_onehot = tf.keras.utils.to_categorical(alz_labels_train_encoded, num_classes)\n",
    "alz_labels_test_onehot = tf.keras.utils.to_categorical(alz_labels_test_encoded, num_classes)\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "#print(alz_labels_train_onehot)\n",
    "\n",
    "print(\"Training labels shape (one-hot encoded):\", alz_labels_train_onehot.shape)\n",
    "print(\"Testing labels shape (one-hot encoded):\", alz_labels_test_onehot.shape)\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "\n",
    "# 0 = MildDemented\n",
    "# 1 = ModerateDemented\n",
    "# 2 = NonDemented\n",
    "# 3 = VeryMildDemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> CNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 119s 737ms/step - loss: 59.5021 - accuracy: 0.6389 - val_loss: 27.4554 - val_accuracy: 0.5004 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 118s 732ms/step - loss: 16.4068 - accuracy: 0.8578 - val_loss: 18.9640 - val_accuracy: 0.3503 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 118s 733ms/step - loss: 8.3665 - accuracy: 0.9053 - val_loss: 8.1105 - val_accuracy: 0.5856 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 117s 728ms/step - loss: 5.6160 - accuracy: 0.9143 - val_loss: 7.9359 - val_accuracy: 0.1939 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 118s 732ms/step - loss: 4.2474 - accuracy: 0.9346 - val_loss: 5.5469 - val_accuracy: 0.6349 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 119s 738ms/step - loss: 3.5474 - accuracy: 0.9248 - val_loss: 4.4119 - val_accuracy: 0.6505 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 118s 731ms/step - loss: 3.0292 - accuracy: 0.9350 - val_loss: 4.1637 - val_accuracy: 0.4918 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 2.5801 - accuracy: 0.9436 - val_loss: 4.4628 - val_accuracy: 0.2909 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 2.3337 - accuracy: 0.9404 - val_loss: 4.1844 - val_accuracy: 0.6388 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 116s 721ms/step - loss: 2.1549 - accuracy: 0.9498 - val_loss: 4.1139 - val_accuracy: 0.4457 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 1.9124 - accuracy: 0.9508 - val_loss: 2.5324 - val_accuracy: 0.6849 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 116s 724ms/step - loss: 1.6950 - accuracy: 0.9590 - val_loss: 4.1693 - val_accuracy: 0.4081 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 1.6028 - accuracy: 0.9594 - val_loss: 3.1133 - val_accuracy: 0.6450 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 117s 730ms/step - loss: 1.5826 - accuracy: 0.9588 - val_loss: 2.3811 - val_accuracy: 0.6708 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 1.4131 - accuracy: 0.9678 - val_loss: 2.6431 - val_accuracy: 0.5903 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 116s 720ms/step - loss: 1.3214 - accuracy: 0.9676 - val_loss: 3.9304 - val_accuracy: 0.4222 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 1.2875 - accuracy: 0.9662 - val_loss: 2.5600 - val_accuracy: 0.5536 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "#THE BEST\n",
    "# Learning rate scheduler - Cyclic Learning Rate\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model_cnn = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.1), kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# history = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "#                      validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "#                      callbacks=[early_stopping, learning_rate_scheduler])\n",
    "\n",
    "\n",
    "\n",
    "history_cyclic_lr = model_cnn.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_cyclic_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 118s 733ms/step - loss: 1.3765 - accuracy: 0.9701 - val_loss: 2.4555 - val_accuracy: 0.6747 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 120s 745ms/step - loss: 1.3446 - accuracy: 0.9664 - val_loss: 2.1095 - val_accuracy: 0.6865 - lr: 9.0000e-05\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 120s 747ms/step - loss: 1.0465 - accuracy: 0.9854 - val_loss: 2.1399 - val_accuracy: 0.6544 - lr: 7.2900e-05\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 117s 729ms/step - loss: 0.8129 - accuracy: 0.9889 - val_loss: 1.7388 - val_accuracy: 0.6740 - lr: 5.3144e-05\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 121s 751ms/step - loss: 0.7143 - accuracy: 0.9951 - val_loss: 1.5777 - val_accuracy: 0.6841 - lr: 3.4868e-05\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 121s 753ms/step - loss: 0.5965 - accuracy: 0.9992 - val_loss: 1.4912 - val_accuracy: 0.7045 - lr: 2.0589e-05\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 121s 754ms/step - loss: 0.5341 - accuracy: 1.0000 - val_loss: 1.4613 - val_accuracy: 0.6943 - lr: 1.0942e-05\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 122s 756ms/step - loss: 0.5075 - accuracy: 0.9996 - val_loss: 1.4883 - val_accuracy: 0.6943 - lr: 5.2335e-06\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 120s 747ms/step - loss: 0.4956 - accuracy: 0.9998 - val_loss: 1.4840 - val_accuracy: 0.7005 - lr: 2.2528e-06\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 117s 725ms/step - loss: 0.4897 - accuracy: 0.9996 - val_loss: 1.4879 - val_accuracy: 0.6982 - lr: 8.7280e-07\n"
     ]
    }
   ],
   "source": [
    "# THE 2ND BEST\n",
    "# Learning rate scheduler - Exponential Decay\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "history_exp_decay = model_cnn.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_exp_decay, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 5s 119ms/step - loss: 2.3811 - accuracy: 0.6708\n",
      "Test Loss: 2.381120443344116\n",
      "Test Accuracy: 0.6708365678787231\n",
      "40/40 [==============================] - 5s 114ms/step\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    MildDemented       0.78      0.17      0.28       179\n",
      "ModerateDemented       0.00      0.00      0.00        12\n",
      "     NonDemented       0.74      0.75      0.74       640\n",
      "VeryMildDemented       0.59      0.77      0.67       448\n",
      "\n",
      "        accuracy                           0.67      1279\n",
      "       macro avg       0.53      0.42      0.42      1279\n",
      "    weighted avg       0.68      0.67      0.65      1279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model_cnn.evaluate(alz_images_test, alz_labels_test_onehot)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = model_cnn.predict(alz_images_test)\n",
    "\n",
    "# Get the categorical names\n",
    "categorical_names = label_encoder.inverse_transform(np.arange(num_classes))\n",
    "\n",
    "# Convert predicted labels from one-hot encoded format to categorical names\n",
    "y_pred_categorical_names = categorical_names[np.argmax(y_pred, axis=1)]\n",
    "test_labels_categorical_names = categorical_names[np.argmax(alz_labels_test_onehot, axis=1)]\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels_categorical_names, y_pred_categorical_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5121, 224, 224)\n",
      "Testing data shape: (1279, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to have rank 4\n",
    "alz_images_train_dnn = alz_images_train.reshape(-1, 224, 224, 1)\n",
    "alz_images_test_dnn = alz_images_test.reshape(-1, 224, 224, 1)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Training data shape:\", alz_images_train.shape)\n",
    "print(\"Testing data shape:\", alz_images_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 28s 173ms/step - loss: 9.0945 - accuracy: 0.4400 - val_loss: 3.8696 - val_accuracy: 0.5020 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 27s 166ms/step - loss: 2.8777 - accuracy: 0.4856 - val_loss: 2.6071 - val_accuracy: 0.5004 - lr: 9.0000e-04\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 28s 174ms/step - loss: 2.0251 - accuracy: 0.4865 - val_loss: 1.8038 - val_accuracy: 0.4676 - lr: 7.2900e-04\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 28s 175ms/step - loss: 1.5940 - accuracy: 0.4889 - val_loss: 1.5805 - val_accuracy: 0.3667 - lr: 5.3144e-04\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 27s 168ms/step - loss: 1.3671 - accuracy: 0.5040 - val_loss: 1.4185 - val_accuracy: 0.5145 - lr: 3.4868e-04\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 28s 174ms/step - loss: 1.2921 - accuracy: 0.4981 - val_loss: 1.3136 - val_accuracy: 0.5223 - lr: 2.0589e-04\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 27s 168ms/step - loss: 1.2258 - accuracy: 0.5162 - val_loss: 1.1726 - val_accuracy: 0.5184 - lr: 1.0942e-04\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 27s 168ms/step - loss: 1.1860 - accuracy: 0.5080 - val_loss: 1.2938 - val_accuracy: 0.5199 - lr: 5.2335e-05\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 28s 175ms/step - loss: 1.1594 - accuracy: 0.5178 - val_loss: 1.2092 - val_accuracy: 0.5184 - lr: 2.2528e-05\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 27s 169ms/step - loss: 1.1427 - accuracy: 0.5152 - val_loss: 1.2389 - val_accuracy: 0.5293 - lr: 8.7280e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model_deep = Sequential([\n",
    "    Flatten(input_shape=(img_height, img_width, num_channels)),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_deep.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,      # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10% of the width\n",
    "    height_shift_range=0.1, # Randomly shift images vertically by up to 10% of the height\n",
    "    shear_range=0.2,        # Randomly apply shear transformations\n",
    "    zoom_range=0.2,         # Randomly zoom in by up to 20%\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    fill_mode='nearest'     # Fill in newly created pixels (due to augmentation) using the nearest existing pixel\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Create augmented training data generator\n",
    "train_generator = datagen.flow(alz_images_train_dnn, alz_labels_train_onehot, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model using the augmented data generator\n",
    "history = model_deep.fit(train_generator,\n",
    "                    steps_per_epoch=len(alz_images_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(alz_images_test_dnn, alz_labels_test_onehot),\n",
    "                    callbacks=[early_stopping, lr_scheduler_exp_decay])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 24ms/step - loss: 1.2253 - accuracy: 0.5082\n",
      "Test Loss: 1.2253220081329346\n",
      "Test Accuracy: 0.5082095265388489\n",
      "40/40 [==============================] - 0s 11ms/step\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    MildDemented       0.00      0.00      0.00       179\n",
      "ModerateDemented       0.00      0.00      0.00        12\n",
      "     NonDemented       0.54      0.88      0.67       640\n",
      "VeryMildDemented       0.38      0.19      0.25       448\n",
      "\n",
      "        accuracy                           0.51      1279\n",
      "       macro avg       0.23      0.27      0.23      1279\n",
      "    weighted avg       0.40      0.51      0.42      1279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model_deep.evaluate(alz_images_test_dnn, alz_labels_test_onehot)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = model_deep.predict(alz_images_test_dnn)\n",
    "\n",
    "# Get the categorical names\n",
    "categorical_names = label_encoder.inverse_transform(np.arange(num_classes))\n",
    "\n",
    "# Convert predicted labels from one-hot encoded format to categorical names\n",
    "y_pred_categorical_names = categorical_names[np.argmax(y_pred, axis=1)]\n",
    "test_labels_categorical_names = categorical_names[np.argmax(alz_labels_test_onehot, axis=1)]\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels_categorical_names, y_pred_categorical_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "320/320 [==============================] - 52s 163ms/step - loss: 2.1412 - accuracy: 0.3033 - val_loss: 1.2253 - val_accuracy: 0.5082 - lr: 8.7280e-06\n",
      "Epoch 2/30\n",
      "320/320 [==============================] - 52s 162ms/step - loss: 1.9144 - accuracy: 0.3287 - val_loss: 1.2675 - val_accuracy: 0.4957 - lr: 7.8552e-06\n",
      "Epoch 3/30\n",
      "320/320 [==============================] - 53s 165ms/step - loss: 1.7933 - accuracy: 0.3542 - val_loss: 1.4209 - val_accuracy: 0.5106 - lr: 6.3627e-06\n",
      "Epoch 4/30\n",
      "320/320 [==============================] - 52s 164ms/step - loss: 1.7213 - accuracy: 0.3687 - val_loss: 1.5053 - val_accuracy: 0.5160 - lr: 4.6384e-06\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Create the oversampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply oversampling to your training data\n",
    "alz_train_balanced_mod, alz_train_balanced_label_mod = oversampler.fit_resample(alz_images_train_dnn.reshape(-1, img_height * img_width * num_channels), alz_labels_train_encoded)\n",
    "\n",
    "# Convert labels back to one-hot encoded format\n",
    "alz_train_balanced_onehot = tf.keras.utils.to_categorical(alz_train_balanced_label_mod, num_classes)\n",
    "\n",
    "# Create augmented training data generator\n",
    "train_generator = datagen.flow(alz_train_balanced_mod.reshape(-1, img_height, img_width, num_channels), alz_train_balanced_onehot, batch_size=batch_size)\n",
    "\n",
    "# Train the model using the augmented data generator\n",
    "history = model_deep.fit(train_generator,\n",
    "                    steps_per_epoch=len(alz_train_balanced_mod) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(alz_images_test_dnn, alz_labels_test_onehot),\n",
    "                    callbacks=[early_stopping, lr_scheduler_exp_decay])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN with Graph based features</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 1s 596ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 1s 713ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generator function to yield batches of preprocessed images\n",
    "def image_generator(images, batch_size=32):\n",
    "    num_images = len(images)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, num_images)\n",
    "        \n",
    "        # Load and preprocess images for the current batch\n",
    "        batch_images = np.stack((images[start_index:end_index],) * 3, axis=-1)\n",
    "        preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(batch_images)\n",
    "        \n",
    "        yield preprocessed_images\n",
    "\n",
    "# Function to extract features from images using a pre-trained CNN and perform PCA\n",
    "def extract_features(images, batch_size=32, n_components=64):\n",
    "    feature_extractor = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    features = []\n",
    "    for batch_images in image_generator(images, batch_size=batch_size):\n",
    "        batch_features = feature_extractor.predict(batch_images)\n",
    "        batch_features_flat = batch_features.reshape(batch_features.shape[0], -1)\n",
    "        features.append(batch_features_flat)\n",
    "    \n",
    "    all_features = np.concatenate(features, axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "    \n",
    "    return reduced_features\n",
    "\n",
    "# Extract features from training and test images\n",
    "train_features = extract_features(alz_images_train)\n",
    "test_features = extract_features(alz_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute adjacency matrix based on feature similarity\n",
    "def compute_feature_similarity(features):\n",
    "    num_images = features.shape[0]\n",
    "    similarities = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarities[i, j] = cosine_similarity(features[i].reshape(1, -1), features[j].reshape(1, -1))[0, 0]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature similarities for training and test images\n",
    "train_feature_similarity = compute_feature_similarity(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_similarity = compute_feature_similarity(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.8\n",
    "\n",
    "# Construct adjacency matrix based on feature similarity\n",
    "def construct_adjacency_matrix(feature_similarity, threshold):\n",
    "    num_images = feature_similarity.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Set adjacency matrix value based on whether feature similarity is above threshold\n",
    "            if feature_similarity[i, j] >= threshold:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1  # Symmetric adjacency matrix\n",
    "    return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for training images: (5121, 5121)\n"
     ]
    }
   ],
   "source": [
    "# Compute adjacency matrix for training and test images\n",
    "train_adj_matrix = construct_adjacency_matrix(train_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for training images:\", train_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for test images: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "test_adj_matrix = construct_adjacency_matrix(test_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for test images:\", test_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features: (5121, 64)\n",
      "Shape of test_features: (1279, 64)\n",
      "\n",
      "Shape of train_features similarity: (5121, 5121)\n",
      "Shape of test_features similarity: (1279, 1279)\n",
      "\n",
      "Shape of train_adj_matrix: (5121, 5121)\n",
      "Shape of test_adj_matrix: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_features:\", train_features.shape)\n",
    "print(\"Shape of test_features:\", test_features.shape)\n",
    "\n",
    "print(\"\\nShape of train_features similarity:\", train_feature_similarity.shape)\n",
    "print(\"Shape of test_features similarity:\", test_feature_similarity.shape)\n",
    "\n",
    "print(\"\\nShape of train_adj_matrix:\", train_adj_matrix.shape)\n",
    "print(\"Shape of test_adj_matrix:\", test_adj_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images with grayscale index: (5121, 224, 224, 1)\n",
      "Shape of testing images with grayscale index: (1279, 224, 224, 1)\n",
      "Shape of training features with graph: (5121, 5185)\n",
      "Shape of testing features with graph: (1279, 1343)\n"
     ]
    }
   ],
   "source": [
    "# Images with grayscale index\n",
    "alz_images_train_with_grayscale_index = alz_images_train[..., np.newaxis]  # Add channel dimension for grayscale images\n",
    "alz_images_test_with_grayscale_index = alz_images_test[..., np.newaxis]\n",
    "\n",
    "# Integrate graph-based features\n",
    "train_features_with_graph = np.concatenate([train_features, train_adj_matrix], axis=1)\n",
    "test_features_with_graph = np.concatenate([test_features, test_adj_matrix], axis=1)\n",
    "\n",
    "print(\"Shape of training images with grayscale index:\", alz_images_train_with_grayscale_index.shape)\n",
    "print(\"Shape of testing images with grayscale index:\", alz_images_test_with_grayscale_index.shape)\n",
    "print(\"Shape of training features with graph:\", train_features_with_graph.shape)\n",
    "print(\"Shape of testing features with graph:\", test_features_with_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded test_adj_matrix: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad the test adjacency matrix with zeros to match the shape of the train adjacency matrix\n",
    "max_nodes = train_adj_matrix.shape[1]\n",
    "test_adj_matrix_padded = np.pad(test_adj_matrix, ((0, 0), (0, max_nodes - test_adj_matrix.shape[1])), mode='constant')\n",
    "\n",
    "# Verify the shape of the padded test adjacency matrix\n",
    "print(\"Shape of padded test_adj_matrix:\", test_adj_matrix_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1))(image_input)\n",
    "conv1_bn = BatchNormalization()(conv1)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1_bn)\n",
    "\n",
    "\n",
    "conv2 = Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool1)\n",
    "conv2_bn = BatchNormalization()(conv2)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2_bn)\n",
    "\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool2)\n",
    "conv3_bn = BatchNormalization()(conv3)\n",
    "maxpool3 = MaxPooling2D(pool_size=(2, 2))(conv3_bn)\n",
    "\n",
    "\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex_most_hard = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex_most_hard.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 119s 732ms/step - loss: 15.5428 - accuracy: 0.5110 - val_loss: 13.4707 - val_accuracy: 0.2995 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 117s 729ms/step - loss: 10.2522 - accuracy: 0.5700 - val_loss: 11.5304 - val_accuracy: 0.0743 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 117s 728ms/step - loss: 7.6901 - accuracy: 0.6206 - val_loss: 7.4679 - val_accuracy: 0.5020 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 117s 729ms/step - loss: 5.9401 - accuracy: 0.6825 - val_loss: 5.6892 - val_accuracy: 0.5434 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 117s 727ms/step - loss: 4.7437 - accuracy: 0.7053 - val_loss: 4.5911 - val_accuracy: 0.6020 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 117s 726ms/step - loss: 3.8584 - accuracy: 0.7448 - val_loss: 4.2190 - val_accuracy: 0.5465 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 3.2983 - accuracy: 0.7536 - val_loss: 3.4480 - val_accuracy: 0.6169 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 2.7880 - accuracy: 0.7905 - val_loss: 3.0535 - val_accuracy: 0.6145 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 117s 724ms/step - loss: 2.3923 - accuracy: 0.8321 - val_loss: 3.0371 - val_accuracy: 0.5934 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 2.0724 - accuracy: 0.8520 - val_loss: 2.6565 - val_accuracy: 0.6294 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 1.8746 - accuracy: 0.8561 - val_loss: 2.5066 - val_accuracy: 0.6185 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 1.6459 - accuracy: 0.8780 - val_loss: 2.2271 - val_accuracy: 0.6357 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 1.4897 - accuracy: 0.8826 - val_loss: 2.5179 - val_accuracy: 0.6091 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 1.5383 - accuracy: 0.8740 - val_loss: 2.2071 - val_accuracy: 0.6153 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 114s 711ms/step - loss: 1.3518 - accuracy: 0.8983 - val_loss: 2.4144 - val_accuracy: 0.6075 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 115s 716ms/step - loss: 1.2232 - accuracy: 0.9143 - val_loss: 2.0389 - val_accuracy: 0.6020 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 116s 718ms/step - loss: 1.1331 - accuracy: 0.9242 - val_loss: 1.8312 - val_accuracy: 0.6411 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 115s 713ms/step - loss: 1.0821 - accuracy: 0.9291 - val_loss: 2.0622 - val_accuracy: 0.6200 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "161/161 [==============================] - 116s 718ms/step - loss: 0.9732 - accuracy: 0.9442 - val_loss: 1.8301 - val_accuracy: 0.6177 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 0.9305 - accuracy: 0.9399 - val_loss: 1.9791 - val_accuracy: 0.6231 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 0.9347 - accuracy: 0.9414 - val_loss: 1.7881 - val_accuracy: 0.6310 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "161/161 [==============================] - 116s 722ms/step - loss: 0.8712 - accuracy: 0.9500 - val_loss: 1.9800 - val_accuracy: 0.6192 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "161/161 [==============================] - 116s 719ms/step - loss: 0.8220 - accuracy: 0.9481 - val_loss: 1.7369 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "161/161 [==============================] - 115s 717ms/step - loss: 0.7659 - accuracy: 0.9553 - val_loss: 1.6392 - val_accuracy: 0.6482 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "161/161 [==============================] - 116s 720ms/step - loss: 0.7183 - accuracy: 0.9584 - val_loss: 1.8521 - val_accuracy: 0.6263 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "161/161 [==============================] - 115s 713ms/step - loss: 0.7086 - accuracy: 0.9615 - val_loss: 1.8738 - val_accuracy: 0.6169 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "161/161 [==============================] - 114s 709ms/step - loss: 0.8091 - accuracy: 0.9342 - val_loss: 1.8068 - val_accuracy: 0.6231 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model_complex_most_hard.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 5s 115ms/step - loss: 1.6392 - accuracy: 0.6482\n",
      "40/40 [==============================] - 4s 94ms/step\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    MildDemented       0.67      0.32      0.44       179\n",
      "ModerateDemented       1.00      0.17      0.29        12\n",
      "     NonDemented       0.67      0.82      0.74       640\n",
      "VeryMildDemented       0.60      0.54      0.57       448\n",
      "\n",
      "        accuracy                           0.65      1279\n",
      "       macro avg       0.74      0.46      0.51      1279\n",
      "    weighted avg       0.65      0.65      0.63      1279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_loss, test_accuracy = model_complex_most_hard.evaluate(\n",
    "    {'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded},\n",
    "    {'output': alz_labels_test_onehot}\n",
    ")\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = model_complex_most_hard.predict(\n",
    "    {'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}\n",
    ")\n",
    "\n",
    "# # Convert predicted labels from one-hot encoded format to categorical labels\n",
    "# y_pred_categorical = np.argmax(y_pred, axis=1)\n",
    "# test_labels_categorical = np.argmax(alz_labels_test_onehot, axis=1)\n",
    "\n",
    "# Get the categorical names\n",
    "categorical_names = label_encoder.inverse_transform(np.arange(num_classes))\n",
    "\n",
    "# Convert predicted labels from one-hot encoded format to categorical names\n",
    "y_pred_categorical_names = categorical_names[np.argmax(y_pred, axis=1)]\n",
    "test_labels_categorical_names = categorical_names[np.argmax(alz_labels_test_onehot, axis=1)]\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels_categorical_names, y_pred_categorical_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jay's Algorthm</h1>\n",
    "SVM and KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Geoffrey's Algorithm</h1>\n",
    "Random Forest and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
