{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys # for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "# def preprocess_image(file_path, img_size):\n",
    "#     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "#     img = cv2.resize(img, img_size)\n",
    "#     img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "def preprocess_image(file_path, img_size):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "    \n",
    "    # Thresholding to remove black background\n",
    "    _, binary_image = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    largest_component_label = np.argmax(stats[1:, cv2.CC_STAT_AREA]) + 1\n",
    "    brain_mask = (labels == largest_component_label).astype(np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(brain_mask)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "target_size =(224, 224)\n",
    "\n",
    "# This function will pull from the directory and all subdirectory for the image and give it a label to the directory it is in\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Iterates through all subdirectories\n",
    "    for subdir in os.listdir(directory):\n",
    "        label = subdir #Make the subdirectory name be a label\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "\n",
    "        # Checks if the object it is looking at is a directory and if it is go into the directory and get all the files and preprocess them\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for image in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, image)\n",
    "\n",
    "                image = preprocess_image(file_path, target_size)\n",
    "\n",
    "                # Append to the arrays after preprocessing\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Image shape: (5121, 224, 224)\n",
      "Labels shape: (5121,)\n",
      "\n",
      "Test\n",
      "Image shape: (1279, 224, 224)\n",
      "Labels shape: (1279,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the directory paths for the training and test datasets\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "# single_test_dir = \"./Alzheimer_s Dataset/single_test\"\n",
    "\n",
    "# Load images and labels from the training directory\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "\n",
    "# Load images and labels from the test directory\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "\n",
    "# alz_single_images_test, alz_single_labels_test = load_images_from_directory(single_test_dir)\n",
    "\n",
    "# Print information about the training dataset\n",
    "print(\"Train\")\n",
    "print('Image shape:', alz_images_train.shape)\n",
    "print('Labels shape:', alz_labels_train.shape)\n",
    "\n",
    "# Print information about the test dataset\n",
    "print(\"\\nTest\")\n",
    "print('Image shape:', alz_images_test.shape)\n",
    "print('Labels shape:', alz_labels_test.shape)\n",
    "\n",
    "\n",
    "# print(\"\\nSingle Test\")\n",
    "# print('Image shape:', alz_single_images_test.shape)\n",
    "# print('Labels shape:', alz_single_labels_test.shape)\n",
    "\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "\n",
    "# print('Image train:', alz_single_images_test) # for debugging\n",
    "\n",
    "# The output of the shape follows this\n",
    "#  (X, X1, X2)\n",
    "# X is the number of pictures in the array   \n",
    "# X1 is the number of rows for a single picture (should be 224 since that is the scale)\n",
    "# X2 is the number of columns in each picture  (should be 224 since that is the scale)\n",
    "#  *Scale can be change to 207 since that is how the data is processed. \n",
    "# \n",
    "# When pull out the full array, you see alot of 0 at the start and end and that is because of the black around the brain\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Aaron's Algorithm </h1>\n",
    "CNN GCNN or similar neural networks that can be adjusted in between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (one-hot encoded): (5121, 4)\n",
      "Testing labels shape (one-hot encoded): (1279, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "alz_labels_train_encoded = label_encoder.fit_transform(alz_labels_train)\n",
    "alz_labels_test_encoded = label_encoder.fit_transform(alz_labels_test)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "alz_labels_train_onehot = tf.keras.utils.to_categorical(alz_labels_train_encoded, num_classes)\n",
    "alz_labels_test_onehot = tf.keras.utils.to_categorical(alz_labels_test_encoded, num_classes)\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "#print(alz_labels_train_onehot)\n",
    "\n",
    "print(\"Training labels shape (one-hot encoded):\", alz_labels_train_onehot.shape)\n",
    "print(\"Testing labels shape (one-hot encoded):\", alz_labels_test_onehot.shape)\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "\n",
    "# 0 = MildDemented\n",
    "# 1 = ModerateDemented\n",
    "# 2 = NonDemented\n",
    "# 3 = VeryMildDemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> CNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 91s 556ms/step - loss: 27.6525 - accuracy: 0.5438 - val_loss: 23.6364 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 17.0131 - accuracy: 0.6259 - val_loss: 29.0601 - val_accuracy: 0.3503 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 89s 554ms/step - loss: 15.9307 - accuracy: 0.6251 - val_loss: 18.6328 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 12.1645 - accuracy: 0.6272 - val_loss: 11.2882 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 89s 551ms/step - loss: 10.0402 - accuracy: 0.6532 - val_loss: 10.6785 - val_accuracy: 0.3511 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 89s 554ms/step - loss: 9.9922 - accuracy: 0.6585 - val_loss: 8.6916 - val_accuracy: 0.5059 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 9.1542 - accuracy: 0.6817 - val_loss: 9.3793 - val_accuracy: 0.3503 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 7.0060 - accuracy: 0.6858 - val_loss: 7.2783 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 5.2502 - accuracy: 0.7049 - val_loss: 10.9590 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 4.3454 - accuracy: 0.7286 - val_loss: 5.2986 - val_accuracy: 0.4801 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 3.7997 - accuracy: 0.7376 - val_loss: 4.3688 - val_accuracy: 0.5887 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 3.7466 - accuracy: 0.7446 - val_loss: 4.4343 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 2.8315 - accuracy: 0.7594 - val_loss: 3.2079 - val_accuracy: 0.5278 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 2.4877 - accuracy: 0.7754 - val_loss: 2.4297 - val_accuracy: 0.6443 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 2.3518 - accuracy: 0.7692 - val_loss: 3.4560 - val_accuracy: 0.4676 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 2.2917 - accuracy: 0.7950 - val_loss: 2.5672 - val_accuracy: 0.6247 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 89s 555ms/step - loss: 1.3074 - accuracy: 0.8871 - val_loss: 1.5612 - val_accuracy: 0.6341 - lr: 2.0000e-04\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 89s 553ms/step - loss: 0.7192 - accuracy: 0.9479 - val_loss: 1.7037 - val_accuracy: 0.6169 - lr: 2.0000e-04\n",
      "Epoch 19/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.5964 - accuracy: 0.9529 - val_loss: 1.4155 - val_accuracy: 0.6247 - lr: 2.0000e-04\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 89s 556ms/step - loss: 0.5880 - accuracy: 0.9516 - val_loss: 1.3955 - val_accuracy: 0.6443 - lr: 2.0000e-04\n",
      "Epoch 21/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.5591 - accuracy: 0.9563 - val_loss: 1.5358 - val_accuracy: 0.6443 - lr: 2.0000e-04\n",
      "Epoch 22/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 0.5892 - accuracy: 0.9498 - val_loss: 1.5795 - val_accuracy: 0.6411 - lr: 2.0000e-04\n",
      "Epoch 23/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.4709 - accuracy: 0.9766 - val_loss: 1.3453 - val_accuracy: 0.6552 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "161/161 [==============================] - 90s 556ms/step - loss: 0.3461 - accuracy: 0.9865 - val_loss: 1.3467 - val_accuracy: 0.6560 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.2934 - accuracy: 0.9889 - val_loss: 1.2582 - val_accuracy: 0.6708 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "161/161 [==============================] - 90s 556ms/step - loss: 0.2940 - accuracy: 0.9859 - val_loss: 1.3571 - val_accuracy: 0.6646 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "161/161 [==============================] - 90s 558ms/step - loss: 0.2834 - accuracy: 0.9855 - val_loss: 1.4201 - val_accuracy: 0.6505 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.3191 - accuracy: 0.9797 - val_loss: 1.4138 - val_accuracy: 0.6693 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#3RD BEST MODEL\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                     validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                     callbacks=[early_stopping, learning_rate_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 92s 565ms/step - loss: 27.6656 - accuracy: 0.5167 - val_loss: 20.6026 - val_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 17.7674 - accuracy: 0.5536 - val_loss: 21.7103 - val_accuracy: 0.3503 - lr: 9.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 12.3870 - accuracy: 0.6091 - val_loss: 9.8986 - val_accuracy: 0.3503 - lr: 7.2900e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 91s 564ms/step - loss: 8.4417 - accuracy: 0.6516 - val_loss: 8.3055 - val_accuracy: 0.3503 - lr: 5.3144e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 4.7702 - accuracy: 0.7389 - val_loss: 4.7302 - val_accuracy: 0.3729 - lr: 3.4868e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 2.4933 - accuracy: 0.8377 - val_loss: 2.8808 - val_accuracy: 0.5395 - lr: 2.0589e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 92s 573ms/step - loss: 1.4114 - accuracy: 0.9291 - val_loss: 1.8714 - val_accuracy: 0.5794 - lr: 1.0942e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 92s 572ms/step - loss: 0.8104 - accuracy: 0.9754 - val_loss: 1.3456 - val_accuracy: 0.6443 - lr: 5.2335e-05\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 91s 568ms/step - loss: 0.4930 - accuracy: 0.9973 - val_loss: 1.1137 - val_accuracy: 0.7029 - lr: 2.2528e-05\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 90s 562ms/step - loss: 0.3667 - accuracy: 0.9996 - val_loss: 1.0643 - val_accuracy: 0.7013 - lr: 8.7280e-06\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 0.3264 - accuracy: 0.9996 - val_loss: 1.0964 - val_accuracy: 0.7076 - lr: 3.0433e-06\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 1.1213 - val_accuracy: 0.7084 - lr: 9.5501e-07\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.3087 - accuracy: 1.0000 - val_loss: 1.1284 - val_accuracy: 0.7123 - lr: 2.6972e-07\n",
      "Epoch 1/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 1.3377 - accuracy: 0.9123 - val_loss: 2.4916 - val_accuracy: 0.4324 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 1.0686 - accuracy: 0.9418 - val_loss: 2.4556 - val_accuracy: 0.5801 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.9128 - accuracy: 0.9623 - val_loss: 1.6709 - val_accuracy: 0.6325 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 91s 565ms/step - loss: 0.8884 - accuracy: 0.9609 - val_loss: 2.1240 - val_accuracy: 0.5895 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.8622 - accuracy: 0.9604 - val_loss: 2.0149 - val_accuracy: 0.6145 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 90s 560ms/step - loss: 0.7280 - accuracy: 0.9756 - val_loss: 1.6453 - val_accuracy: 0.6575 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.6862 - accuracy: 0.9746 - val_loss: 1.8803 - val_accuracy: 0.6231 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 91s 566ms/step - loss: 0.7031 - accuracy: 0.9746 - val_loss: 1.6910 - val_accuracy: 0.6615 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 90s 557ms/step - loss: 0.7533 - accuracy: 0.9676 - val_loss: 1.6253 - val_accuracy: 0.6677 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 91s 563ms/step - loss: 0.6380 - accuracy: 0.9799 - val_loss: 1.5865 - val_accuracy: 0.6990 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 91s 562ms/step - loss: 0.6011 - accuracy: 0.9791 - val_loss: 1.5091 - val_accuracy: 0.6677 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.5930 - accuracy: 0.9781 - val_loss: 2.5179 - val_accuracy: 0.6083 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 90s 561ms/step - loss: 0.6175 - accuracy: 0.9772 - val_loss: 1.6544 - val_accuracy: 0.6669 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 90s 559ms/step - loss: 0.6104 - accuracy: 0.9787 - val_loss: 1.5118 - val_accuracy: 0.6959 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# THE 2ND BEST\n",
    "# Learning rate scheduler - Exponential Decay\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "#THE BEST\n",
    "# Learning rate scheduler - Cyclic Learning Rate\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, num_channels), \n",
    "           kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.1), kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# history = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "#                      validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "#                      callbacks=[early_stopping, learning_rate_scheduler])\n",
    "\n",
    "history_exp_decay = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_exp_decay, early_stopping])\n",
    "\n",
    "history_cyclic_lr = model.fit(alz_images_train, alz_labels_train_onehot, epochs=30, batch_size=32,\n",
    "                              validation_data=(alz_images_test, alz_labels_test_onehot), \n",
    "                              callbacks=[lr_scheduler_cyclic_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DNN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5121, 224, 224)\n",
      "Testing data shape: (1279, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to have rank 4\n",
    "alz_images_train_dnn = alz_images_train.reshape(-1, 224, 224, 1)\n",
    "alz_images_test_dnn = alz_images_test.reshape(-1, 224, 224, 1)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Training data shape:\", alz_images_train.shape)\n",
    "print(\"Testing data shape:\", alz_images_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 27s 166ms/step - loss: 8.8108 - accuracy: 0.4496 - val_loss: 3.7299 - val_accuracy: 0.3862 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 2.9792 - accuracy: 0.4826 - val_loss: 3.0124 - val_accuracy: 0.5223 - lr: 9.0000e-04\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 27s 166ms/step - loss: 2.0992 - accuracy: 0.4934 - val_loss: 1.9373 - val_accuracy: 0.3315 - lr: 7.2900e-04\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 26s 162ms/step - loss: 1.6142 - accuracy: 0.5044 - val_loss: 1.4650 - val_accuracy: 0.5317 - lr: 5.3144e-04\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.4068 - accuracy: 0.4926 - val_loss: 1.3298 - val_accuracy: 0.5090 - lr: 3.4868e-04\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 26s 163ms/step - loss: 1.2846 - accuracy: 0.5060 - val_loss: 1.2295 - val_accuracy: 0.4980 - lr: 2.0589e-04\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 26s 163ms/step - loss: 1.2265 - accuracy: 0.5117 - val_loss: 1.1794 - val_accuracy: 0.5192 - lr: 1.0942e-04\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 26s 164ms/step - loss: 1.1915 - accuracy: 0.5099 - val_loss: 1.2036 - val_accuracy: 0.5207 - lr: 5.2335e-05\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.1709 - accuracy: 0.5038 - val_loss: 1.1799 - val_accuracy: 0.5262 - lr: 2.2528e-05\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 26s 165ms/step - loss: 1.1499 - accuracy: 0.5166 - val_loss: 1.2665 - val_accuracy: 0.5231 - lr: 8.7280e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define model architecture with batch normalization\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(img_height, img_width, num_channels)),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,      # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10% of the width\n",
    "    height_shift_range=0.1, # Randomly shift images vertically by up to 10% of the height\n",
    "    shear_range=0.2,        # Randomly apply shear transformations\n",
    "    zoom_range=0.2,         # Randomly zoom in by up to 20%\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    fill_mode='nearest'     # Fill in newly created pixels (due to augmentation) using the nearest existing pixel\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Create augmented training data generator\n",
    "train_generator = datagen.flow(alz_images_train_dnn, alz_labels_train_onehot, batch_size=batch_size)\n",
    "\n",
    "# Train the model using the augmented data generator\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=len(alz_images_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(alz_images_test_dnn, alz_labels_test_onehot),\n",
    "                    callbacks=[early_stopping, lr_scheduler_exp_decay])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>FNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 1s 596ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 1s 713ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generator function to yield batches of preprocessed images\n",
    "def image_generator(images, batch_size=32):\n",
    "    num_images = len(images)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, num_images)\n",
    "        \n",
    "        # Load and preprocess images for the current batch\n",
    "        batch_images = np.stack((images[start_index:end_index],) * 3, axis=-1)\n",
    "        preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(batch_images)\n",
    "        \n",
    "        yield preprocessed_images\n",
    "\n",
    "# Function to extract features from images using a pre-trained CNN and perform PCA\n",
    "def extract_features(images, batch_size=32, n_components=64):\n",
    "    feature_extractor = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    features = []\n",
    "    for batch_images in image_generator(images, batch_size=batch_size):\n",
    "        batch_features = feature_extractor.predict(batch_images)\n",
    "        batch_features_flat = batch_features.reshape(batch_features.shape[0], -1)\n",
    "        features.append(batch_features_flat)\n",
    "    \n",
    "    all_features = np.concatenate(features, axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "    \n",
    "    return reduced_features\n",
    "\n",
    "# Extract features from training and test images\n",
    "train_features = extract_features(alz_images_train)\n",
    "test_features = extract_features(alz_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 1.2813 - accuracy: 0.4634 - val_loss: 1.1530 - val_accuracy: 0.4683\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9980 - accuracy: 0.5397 - val_loss: 1.1772 - val_accuracy: 0.4652\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8847 - accuracy: 0.5864 - val_loss: 1.2199 - val_accuracy: 0.4543\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8280 - accuracy: 0.6178 - val_loss: 1.2544 - val_accuracy: 0.4457\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7919 - accuracy: 0.6362 - val_loss: 1.2830 - val_accuracy: 0.4410\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7651 - accuracy: 0.6467 - val_loss: 1.3191 - val_accuracy: 0.4402\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7349 - accuracy: 0.6624 - val_loss: 1.3687 - val_accuracy: 0.4386\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7152 - accuracy: 0.6714 - val_loss: 1.4054 - val_accuracy: 0.4285\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.6842 - val_loss: 1.4266 - val_accuracy: 0.4269\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6729 - accuracy: 0.7055 - val_loss: 1.4621 - val_accuracy: 0.4363\n",
      "40/40 [==============================] - 0s 821us/step - loss: 1.4621 - accuracy: 0.4363\n",
      "Test Loss: 1.4620929956436157, Test Accuracy: 0.4362783432006836\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the GCN model architecture\n",
    "def create_gcn_model(input_dim, output_dim):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(output_dim, activation='softmax')(x)  # Adjust activation based on your task\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = train_features.shape[1]  # Input dimension is the number of features after PCA\n",
    "output_dim = num_classes  # Output dimension is the number of classes\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_gcn_model(input_dim, output_dim)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_features, alz_labels_train_onehot, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(test_features, alz_labels_test_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_features, alz_labels_test_onehot)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN with Graph based features</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute adjacency matrix based on feature similarity\n",
    "def compute_feature_similarity(features):\n",
    "    num_images = features.shape[0]\n",
    "    similarities = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarities[i, j] = cosine_similarity(features[i].reshape(1, -1), features[j].reshape(1, -1))[0, 0]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature similarities for training and test images\n",
    "train_feature_similarity = compute_feature_similarity(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_similarity = compute_feature_similarity(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.8\n",
    "\n",
    "# Construct adjacency matrix based on feature similarity\n",
    "def construct_adjacency_matrix(feature_similarity, threshold):\n",
    "    num_images = feature_similarity.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Set adjacency matrix value based on whether feature similarity is above threshold\n",
    "            if feature_similarity[i, j] >= threshold:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1  # Symmetric adjacency matrix\n",
    "    return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for training images: (5121, 5121)\n"
     ]
    }
   ],
   "source": [
    "# Compute adjacency matrix for training and test images\n",
    "train_adj_matrix = construct_adjacency_matrix(train_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for training images:\", train_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for test images: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "test_adj_matrix = construct_adjacency_matrix(test_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for test images:\", test_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features: (5121, 64)\n",
      "Shape of test_features: (1279, 64)\n",
      "\n",
      "Shape of train_features similarity: (5121, 5121)\n",
      "Shape of test_features similarity: (1279, 1279)\n",
      "\n",
      "Shape of train_adj_matrix: (5121, 5121)\n",
      "Shape of test_adj_matrix: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_features:\", train_features.shape)\n",
    "print(\"Shape of test_features:\", test_features.shape)\n",
    "\n",
    "print(\"\\nShape of train_features similarity:\", train_feature_similarity.shape)\n",
    "print(\"Shape of test_features similarity:\", test_feature_similarity.shape)\n",
    "\n",
    "print(\"\\nShape of train_adj_matrix:\", train_adj_matrix.shape)\n",
    "print(\"Shape of test_adj_matrix:\", test_adj_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images with grayscale index: (5121, 224, 224, 1)\n",
      "Shape of testing images with grayscale index: (1279, 224, 224, 1)\n",
      "Shape of training features with graph: (5121, 5185)\n",
      "Shape of testing features with graph: (1279, 1343)\n"
     ]
    }
   ],
   "source": [
    "# Images with grayscale index\n",
    "alz_images_train_with_grayscale_index = alz_images_train[..., np.newaxis]  # Add channel dimension for grayscale images\n",
    "alz_images_test_with_grayscale_index = alz_images_test[..., np.newaxis]\n",
    "\n",
    "# Integrate graph-based features\n",
    "train_features_with_graph = np.concatenate([train_features, train_adj_matrix], axis=1)\n",
    "test_features_with_graph = np.concatenate([test_features, test_adj_matrix], axis=1)\n",
    "\n",
    "print(\"Shape of training images with grayscale index:\", alz_images_train_with_grayscale_index.shape)\n",
    "print(\"Shape of testing images with grayscale index:\", alz_images_test_with_grayscale_index.shape)\n",
    "print(\"Shape of training features with graph:\", train_features_with_graph.shape)\n",
    "print(\"Shape of testing features with graph:\", test_features_with_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 50176)        0           ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " graph_input (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " adj_input (InputLayer)         [(None, 5121)]       0           []                               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 55361)        0           ['flatten_10[0][0]',             \n",
      "                                                                  'graph_input[0][0]',            \n",
      "                                                                  'adj_input[0][0]']              \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 128)          7086336     ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 64)           8256        ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 4)            260         ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,094,852\n",
      "Trainable params: 7,094,852\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "flatten_image = Flatten()(image_input)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers\n",
    "x = Dense(128, activation='relu')(concatenated_input)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model2 = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded test_adj_matrix: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad the test adjacency matrix with zeros to match the shape of the train adjacency matrix\n",
    "max_nodes = train_adj_matrix.shape[1]\n",
    "test_adj_matrix_padded = np.pad(test_adj_matrix, ((0, 0), (0, max_nodes - test_adj_matrix.shape[1])), mode='constant')\n",
    "\n",
    "# Verify the shape of the padded test adjacency matrix\n",
    "print(\"Shape of padded test_adj_matrix:\", test_adj_matrix_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 16s 95ms/step - loss: 2.8730 - accuracy: 0.4933 - val_loss: 4.3943 - val_accuracy: 0.3503\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 1.1173 - accuracy: 0.6171 - val_loss: 4.1000 - val_accuracy: 0.5004\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 1.0209 - accuracy: 0.6469 - val_loss: 1.9302 - val_accuracy: 0.3972\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 15s 92ms/step - loss: 0.6391 - accuracy: 0.7424 - val_loss: 1.9182 - val_accuracy: 0.5387\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.6414 - accuracy: 0.7442 - val_loss: 1.2701 - val_accuracy: 0.4910\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 15s 92ms/step - loss: 0.5399 - accuracy: 0.7727 - val_loss: 2.4503 - val_accuracy: 0.3886\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.5566 - accuracy: 0.7834 - val_loss: 1.8231 - val_accuracy: 0.5504\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.4219 - accuracy: 0.8303 - val_loss: 1.5968 - val_accuracy: 0.5426\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.3689 - accuracy: 0.8559 - val_loss: 2.0184 - val_accuracy: 0.5238\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 13s 81ms/step - loss: 0.3930 - accuracy: 0.8403 - val_loss: 2.5522 - val_accuracy: 0.5543\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = model2.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test cnn/graph</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded test_adj_matrix: (1279, 5121)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad the test adjacency matrix with zeros to match the shape of the train adjacency matrix\n",
    "max_nodes = train_adj_matrix.shape[1]\n",
    "test_adj_matrix_padded = np.pad(test_adj_matrix, ((0, 0), (0, max_nodes - test_adj_matrix.shape[1])), mode='constant')\n",
    "\n",
    "# Verify the shape of the padded test adjacency matrix\n",
    "print(\"Shape of padded test_adj_matrix:\", test_adj_matrix_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(image_input)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# Print model summary\n",
    "#model_complex.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 78s 484ms/step - loss: 2.3077 - accuracy: 0.4983 - val_loss: 1.8118 - val_accuracy: 0.5340 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 79s 491ms/step - loss: 1.6609 - accuracy: 0.5456 - val_loss: 1.6519 - val_accuracy: 0.5059 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 84s 525ms/step - loss: 1.4855 - accuracy: 0.5866 - val_loss: 1.5488 - val_accuracy: 0.5223 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 78s 487ms/step - loss: 1.3716 - accuracy: 0.6116 - val_loss: 1.6318 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 79s 489ms/step - loss: 1.2668 - accuracy: 0.6368 - val_loss: 1.4206 - val_accuracy: 0.5450 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 78s 485ms/step - loss: 1.1985 - accuracy: 0.6714 - val_loss: 1.4485 - val_accuracy: 0.4996 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 78s 485ms/step - loss: 1.1083 - accuracy: 0.7213 - val_loss: 1.3745 - val_accuracy: 0.5708 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 79s 490ms/step - loss: 1.0283 - accuracy: 0.7655 - val_loss: 1.5118 - val_accuracy: 0.4676 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 0.9395 - accuracy: 0.8100 - val_loss: 1.5514 - val_accuracy: 0.5731 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 77s 481ms/step - loss: 0.8671 - accuracy: 0.8387 - val_loss: 1.5782 - val_accuracy: 0.5575 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "history = model_complex.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "\n",
    "# Flatten the image data\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(image_input)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.1))(maxpool1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten_image = Flatten()(maxpool2)\n",
    "\n",
    "# Concatenate flattened image data with graph data and adjacency matrix\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers with dropout regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(concatenated_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model_complex2 = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_complex2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# def exponential_decay(epoch, initial_lr=0.001, decay_rate=0.9):\n",
    "#    return initial_lr * np.power(decay_rate, epoch)\n",
    "\n",
    "# lr_scheduler_exp_decay = LearningRateScheduler(exponential_decay)\n",
    "\n",
    "def cyclic_lr(epoch, lr_max=0.001, lr_min=0.0001, step_size=8):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler_cyclic_lr = LearningRateScheduler(cyclic_lr)\n",
    "\n",
    "# Print model summary\n",
    "#model_complex.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "161/161 [==============================] - 82s 507ms/step - loss: 6.1124 - accuracy: 0.4812 - val_loss: 4.6646 - val_accuracy: 0.5145 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 80s 498ms/step - loss: 3.9297 - accuracy: 0.5075 - val_loss: 3.2770 - val_accuracy: 0.5262 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 80s 497ms/step - loss: 2.8328 - accuracy: 0.5155 - val_loss: 2.4607 - val_accuracy: 0.5106 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 80s 499ms/step - loss: 2.1697 - accuracy: 0.5310 - val_loss: 1.9489 - val_accuracy: 0.5137 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 79s 494ms/step - loss: 1.7794 - accuracy: 0.5298 - val_loss: 1.6577 - val_accuracy: 0.5223 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 80s 498ms/step - loss: 1.5427 - accuracy: 0.5310 - val_loss: 1.4876 - val_accuracy: 0.5246 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 79s 493ms/step - loss: 1.3887 - accuracy: 0.5456 - val_loss: 1.3998 - val_accuracy: 0.5121 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 1.2991 - accuracy: 0.5483 - val_loss: 1.3006 - val_accuracy: 0.5278 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 79s 494ms/step - loss: 1.2185 - accuracy: 0.5632 - val_loss: 1.3055 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 79s 492ms/step - loss: 1.1653 - accuracy: 0.5729 - val_loss: 1.2372 - val_accuracy: 0.5238 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 82s 510ms/step - loss: 1.1217 - accuracy: 0.5819 - val_loss: 1.2249 - val_accuracy: 0.5309 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 83s 514ms/step - loss: 1.0848 - accuracy: 0.5958 - val_loss: 1.1952 - val_accuracy: 0.5231 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 83s 514ms/step - loss: 1.0567 - accuracy: 0.6102 - val_loss: 1.2047 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 83s 518ms/step - loss: 1.0277 - accuracy: 0.6182 - val_loss: 1.2223 - val_accuracy: 0.5215 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 83s 515ms/step - loss: 1.0065 - accuracy: 0.6296 - val_loss: 1.2196 - val_accuracy: 0.4902 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "# Fit the model to the training data\n",
    "history2 = model_complex2.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix_padded}, {'output': alz_labels_test_onehot}),\n",
    "    callbacks=[lr_scheduler_cyclic_lr, early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jay's Algorthm</h1>\n",
    "SVM and KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Geoffrey's Algorithm</h1>\n",
    "Random Forest and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
