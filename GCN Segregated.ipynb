{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys # for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "# def preprocess_image(file_path, img_size):\n",
    "#     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "#     img = cv2.resize(img, img_size)\n",
    "#     img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function preprocesses the image by reading in the image apply grayscale make all the sizes the same and \n",
    "def preprocess_image(file_path, img_size):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # Grayscale will even the playing field if we start getting different types of images. If the images color is a factor we can take out grayscale\n",
    "    \n",
    "    # Thresholding to remove black background\n",
    "    _, binary_image = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    largest_component_label = np.argmax(stats[1:, cv2.CC_STAT_AREA]) + 1\n",
    "    brain_mask = (labels == largest_component_label).astype(np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(brain_mask)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img.astype('float')/255.0 # Make the pixels become float and normalize to 0-1 for normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "target_size =(224, 224)\n",
    "\n",
    "# This function will pull from the directory and all subdirectory for the image and give it a label to the directory it is in\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Iterates through all subdirectories\n",
    "    for subdir in os.listdir(directory):\n",
    "        label = subdir #Make the subdirectory name be a label\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "\n",
    "        # Checks if the object it is looking at is a directory and if it is go into the directory and get all the files and preprocess them\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for image in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, image)\n",
    "\n",
    "                image = preprocess_image(file_path, target_size)\n",
    "\n",
    "                # Append to the arrays after preprocessing\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Image shape: (5121, 224, 224)\n",
      "Labels shape: (5121,)\n",
      "\n",
      "Test\n",
      "Image shape: (1279, 224, 224)\n",
      "Labels shape: (1279,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the directory paths for the training and test datasets\n",
    "train_dir = \"./Alzheimer_s Dataset/train\"\n",
    "test_dir = \"./Alzheimer_s Dataset/test\"\n",
    "# single_test_dir = \"./Alzheimer_s Dataset/single_test\"\n",
    "\n",
    "# Load images and labels from the training directory\n",
    "alz_images_train, alz_labels_train = load_images_from_directory(train_dir)\n",
    "\n",
    "# Load images and labels from the test directory\n",
    "alz_images_test, alz_labels_test = load_images_from_directory(test_dir)\n",
    "\n",
    "# alz_single_images_test, alz_single_labels_test = load_images_from_directory(single_test_dir)\n",
    "\n",
    "# Print information about the training dataset\n",
    "print(\"Train\")\n",
    "print('Image shape:', alz_images_train.shape)\n",
    "print('Labels shape:', alz_labels_train.shape)\n",
    "\n",
    "# Print information about the test dataset\n",
    "print(\"\\nTest\")\n",
    "print('Image shape:', alz_images_test.shape)\n",
    "print('Labels shape:', alz_labels_test.shape)\n",
    "\n",
    "\n",
    "# print(\"\\nSingle Test\")\n",
    "# print('Image shape:', alz_single_images_test.shape)\n",
    "# print('Labels shape:', alz_single_labels_test.shape)\n",
    "\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "\n",
    "# print('Image train:', alz_single_images_test) # for debugging\n",
    "\n",
    "# The output of the shape follows this\n",
    "#  (X, X1, X2)\n",
    "# X is the number of pictures in the array   \n",
    "# X1 is the number of rows for a single picture (should be 224 since that is the scale)\n",
    "# X2 is the number of columns in each picture  (should be 224 since that is the scale)\n",
    "#  *Scale can be change to 207 since that is how the data is processed. \n",
    "# \n",
    "# When pull out the full array, you see alot of 0 at the start and end and that is because of the black around the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (one-hot encoded): (5121, 4)\n",
      "Testing labels shape (one-hot encoded): (1279, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "alz_labels_train_encoded = label_encoder.fit_transform(alz_labels_train)\n",
    "alz_labels_test_encoded = label_encoder.fit_transform(alz_labels_test)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "alz_labels_train_onehot = tf.keras.utils.to_categorical(alz_labels_train_encoded, num_classes)\n",
    "alz_labels_test_onehot = tf.keras.utils.to_categorical(alz_labels_test_encoded, num_classes)\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize) # for debugging\n",
    "#print(alz_labels_train_onehot)\n",
    "\n",
    "print(\"Training labels shape (one-hot encoded):\", alz_labels_train_onehot.shape)\n",
    "print(\"Testing labels shape (one-hot encoded):\", alz_labels_test_onehot.shape)\n",
    "\n",
    "# print('Image train:', alz_images_train) # for debugging\n",
    "\n",
    "\n",
    "# 0 = MildDemented\n",
    "# 1 = ModerateDemented\n",
    "# 2 = NonDemented\n",
    "# 3 = VeryMildDemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = target_size[1]\n",
    "img_width = target_size[0]\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GCN preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 669ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 1s 721ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generator function to yield batches of preprocessed images\n",
    "def image_generator(images, batch_size=32):\n",
    "    num_images = len(images)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, num_images)\n",
    "        \n",
    "        # Load and preprocess images for the current batch\n",
    "        batch_images = np.stack((images[start_index:end_index],) * 3, axis=-1)\n",
    "        preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(batch_images)\n",
    "        \n",
    "        yield preprocessed_images\n",
    "\n",
    "# Function to extract features from images using a pre-trained CNN and perform PCA\n",
    "def extract_features(images, batch_size=32, n_components=64):\n",
    "    feature_extractor = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    features = []\n",
    "    for batch_images in image_generator(images, batch_size=batch_size):\n",
    "        batch_features = feature_extractor.predict(batch_images)\n",
    "        batch_features_flat = batch_features.reshape(batch_features.shape[0], -1)\n",
    "        features.append(batch_features_flat)\n",
    "    \n",
    "    all_features = np.concatenate(features, axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(all_features)\n",
    "    \n",
    "    return reduced_features\n",
    "\n",
    "# Extract features from training and test images\n",
    "train_features = extract_features(alz_images_train)\n",
    "test_features = extract_features(alz_images_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute adjacency matrix based on feature similarity\n",
    "def compute_feature_similarity(features):\n",
    "    num_images = features.shape[0]\n",
    "    similarities = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarities[i, j] = cosine_similarity(features[i].reshape(1, -1), features[j].reshape(1, -1))[0, 0]\n",
    "    return similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature similarities for training and test images\n",
    "train_feature_similarity = compute_feature_similarity(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_similarity = compute_feature_similarity(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.8\n",
    "\n",
    "# Construct adjacency matrix based on feature similarity\n",
    "def construct_adjacency_matrix(feature_similarity, threshold):\n",
    "    num_images = feature_similarity.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_images, num_images))\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            # Set adjacency matrix value based on whether feature similarity is above threshold\n",
    "            if feature_similarity[i, j] >= threshold:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1  # Symmetric adjacency matrix\n",
    "    return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for training images: (5121, 5121)\n"
     ]
    }
   ],
   "source": [
    "# Compute adjacency matrix for training and test images\n",
    "train_adj_matrix = construct_adjacency_matrix(train_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for training images:\", train_adj_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix for test images: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "test_adj_matrix = construct_adjacency_matrix(test_feature_similarity, similarity_threshold)\n",
    "print(\"Shape of adjacency matrix for test images:\", test_adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features: (5121, 64)\n",
      "Shape of test_features: (1279, 64)\n",
      "\n",
      "Shape of train_features similarity: (5121, 5121)\n",
      "Shape of test_features similarity: (1279, 1279)\n",
      "\n",
      "Shape of train_adj_matrix: (5121, 5121)\n",
      "Shape of test_adj_matrix: (1279, 1279)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of train_features:\", train_features.shape)\n",
    "print(\"Shape of test_features:\", test_features.shape)\n",
    "\n",
    "print(\"\\nShape of train_features similarity:\", train_feature_similarity.shape)\n",
    "print(\"Shape of test_features similarity:\", test_feature_similarity.shape)\n",
    "\n",
    "print(\"\\nShape of train_adj_matrix:\", train_adj_matrix.shape)\n",
    "print(\"Shape of test_adj_matrix:\", test_adj_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images with grayscale index: (5121, 224, 224, 1)\n",
      "Shape of testing images with grayscale index: (1279, 224, 224, 1)\n",
      "Shape of training features with graph: (5121, 5185)\n",
      "Shape of testing features with graph: (1279, 1343)\n"
     ]
    }
   ],
   "source": [
    "# Images with grayscale index\n",
    "alz_images_train_with_grayscale_index = alz_images_train[..., np.newaxis]  # Add channel dimension for grayscale images\n",
    "alz_images_test_with_grayscale_index = alz_images_test[..., np.newaxis]\n",
    "\n",
    "# Integrate graph-based features\n",
    "train_features_with_graph = np.concatenate([train_features, train_adj_matrix], axis=1)\n",
    "test_features_with_graph = np.concatenate([test_features, test_adj_matrix], axis=1)\n",
    "\n",
    "print(\"Shape of training images with grayscale index:\", alz_images_train_with_grayscale_index.shape)\n",
    "print(\"Shape of testing images with grayscale index:\", alz_images_test_with_grayscale_index.shape)\n",
    "print(\"Shape of training features with graph:\", train_features_with_graph.shape)\n",
    "print(\"Shape of testing features with graph:\", test_features_with_graph.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN with graph based features </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 5121), dtype=tf.float32, name='adj_input'), name='adj_input', description=\"created by layer 'adj_input'\")\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 50176)        0           ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " graph_input (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 50240)        0           ['flatten_5[0][0]',              \n",
      "                                                                  'graph_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 128)          6430848     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 64)           8256        ['dense_63[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 4)            260         ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,439,364\n",
      "Trainable params: 6,439,364\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Define input layers for image data and graph data\n",
    "image_input = Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "graph_input = Input(shape=(train_features.shape[1],), name='graph_input')\n",
    "adj_input = Input(shape=(train_adj_matrix.shape[1],), name='adj_input')\n",
    "print(adj_input)\n",
    "\n",
    "# Flatten the image data\n",
    "flatten_image = Flatten()(image_input)\n",
    "\n",
    "# Concatenate flattened image data with graph data\n",
    "concatenated_input = Concatenate()([flatten_image, graph_input, adj_input])\n",
    "\n",
    "# Define the dense layers\n",
    "x = Dense(128, activation='relu')(concatenated_input)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(4, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create the model\n",
    "model2 = Model(inputs=[image_input, graph_input, adj_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['adj_input'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 16s 91ms/step - loss: 4.7009 - accuracy: 0.4716 - val_loss: 1.5796 - val_accuracy: 0.5387\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 14s 88ms/step - loss: 0.9648 - accuracy: 0.6249 - val_loss: 3.1012 - val_accuracy: 0.5020\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 14s 85ms/step - loss: 0.8954 - accuracy: 0.6616 - val_loss: 2.3167 - val_accuracy: 0.3612\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.8329 - accuracy: 0.6848 - val_loss: 3.9748 - val_accuracy: 0.1650\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 14s 89ms/step - loss: 0.8538 - accuracy: 0.6633 - val_loss: 2.3657 - val_accuracy: 0.5012\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 15s 91ms/step - loss: 0.6276 - accuracy: 0.7405 - val_loss: 1.6394 - val_accuracy: 0.5536\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 14s 88ms/step - loss: 0.5937 - accuracy: 0.7479 - val_loss: 1.5371 - val_accuracy: 0.5590\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 14s 90ms/step - loss: 0.5262 - accuracy: 0.7770 - val_loss: 1.4197 - val_accuracy: 0.5442\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.5805 - accuracy: 0.7612 - val_loss: 1.3547 - val_accuracy: 0.4425\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.5307 - accuracy: 0.7784 - val_loss: 3.6994 - val_accuracy: 0.5004\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    {'image_input': alz_images_train_with_grayscale_index, 'graph_input': train_features, 'adj_input': train_adj_matrix},\n",
    "    {'output': alz_labels_train_onehot},\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=({'image_input': alz_images_test_with_grayscale_index, 'graph_input': test_features, 'adj_input': test_adj_matrix}, {'output': alz_labels_test_onehot})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>FNN</h1>\n",
    "still need to adjust a bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 1.3894 - accuracy: 0.4583 - val_loss: 1.2713 - val_accuracy: 0.4496\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0049 - accuracy: 0.5558 - val_loss: 1.2862 - val_accuracy: 0.4464\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9077 - accuracy: 0.5948 - val_loss: 1.2831 - val_accuracy: 0.4371\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8381 - accuracy: 0.6335 - val_loss: 1.2883 - val_accuracy: 0.4425\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8034 - accuracy: 0.6407 - val_loss: 1.3201 - val_accuracy: 0.4347\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7784 - accuracy: 0.6493 - val_loss: 1.3408 - val_accuracy: 0.4308\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7555 - accuracy: 0.6563 - val_loss: 1.3790 - val_accuracy: 0.4167\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7270 - accuracy: 0.6774 - val_loss: 1.4060 - val_accuracy: 0.4159\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7083 - accuracy: 0.6786 - val_loss: 1.4273 - val_accuracy: 0.4175\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.6995 - val_loss: 1.4663 - val_accuracy: 0.4285\n",
      "40/40 [==============================] - 0s 719us/step - loss: 1.4663 - accuracy: 0.4285\n",
      "Test Loss: 1.4663267135620117, Test Accuracy: 0.4284597337245941\n"
     ]
    }
   ],
   "source": [
    "# Define the GCN model architecture\n",
    "def create_gcn_model(input_dim, output_dim):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(output_dim, activation='softmax')(x)  # Adjust activation based on your task\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = train_features.shape[1]  # Input dimension is the number of features after PCA\n",
    "output_dim = num_classes  # Output dimension is the number of classes\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_gcn_model(input_dim, output_dim)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_features, alz_labels_train_onehot, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(test_features, alz_labels_test_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_features, alz_labels_test_onehot)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
